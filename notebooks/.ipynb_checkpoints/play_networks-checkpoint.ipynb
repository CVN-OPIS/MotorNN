{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.signal import resample\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle \n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(quant, minn, maxx):\n",
    "    a = -1\n",
    "    b = 1\n",
    "    t = a + ( quant - minn) * ((b - a) / (maxx - minn))\n",
    "    return t.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFIntNet(nn.Module):\n",
    "    def __init__(self, input_dim=15, output_dim=1, act='tanh'):\n",
    "        super(FFIntNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 16, bias=False)\n",
    "        self.linear2 = nn.Linear(16, 17, bias=False)\n",
    "        self.linear3 = nn.Linear(17, 16, bias=False)\n",
    "        self.linear4 = nn.Linear(16, 5, bias=False)\n",
    "        self.linear5 = nn.Linear(5, output_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        if act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        if act == 'tanh':\n",
    "            self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.linear2(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.linear5(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('../../datasets/raw_data_integer.pkl','rb')\n",
    "dataset = pickle.load(fin)\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_samples(dataset, window, stride):\n",
    "    train_samples = []\n",
    "    for k in list(dataset.keys())[:8]:\n",
    "        for i in range(0,dataset[k].shape[1],stride):\n",
    "            if i + window < dataset[k].shape[1]:\n",
    "                train_samples.append([k,i,i+window])\n",
    "    \n",
    "    val_samples = []\n",
    "    for k in list(dataset.keys())[8:]:\n",
    "        for i in range(0,dataset[k].shape[1],stride):\n",
    "            if i + window < dataset[k].shape[1]:\n",
    "                val_samples.append([k,i,i+window])\n",
    "    \n",
    "    return train_samples, val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples :  194703\n",
      "val samples :  102579\n"
     ]
    }
   ],
   "source": [
    "train_samples, val_samples = get_train_val_samples(dataset, 5, 1)\n",
    "print ('train samples : ',len(train_samples))\n",
    "print ('val samples : ',len(val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869\n"
     ]
    }
   ],
   "source": [
    "net =  FFIntNet().cuda()\n",
    "print (sum(p.numel() for p in net.parameters()))\n",
    "# inp = torch.randn(1024,15).cuda()\n",
    "# net(inp).size()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0 train loss :  0.09644579064315527  val loss :  0.15467525766924534\n",
      "epoch :  1 train loss :  0.09101548439857224  val loss :  0.14893036332200205\n",
      "epoch :  2 train loss :  0.08676038097336654  val loss :  0.14353235092879696\n",
      "epoch :  3 train loss :  0.08431844963297169  val loss :  0.1382799257335857\n",
      "epoch :  4 train loss :  0.08018125258191093  val loss :  0.13317468931920376\n",
      "epoch :  5 train loss :  0.07787435170243548  val loss :  0.12833949924293342\n",
      "epoch :  6 train loss :  0.07579121857647496  val loss :  0.12398037363949121\n",
      "epoch :  7 train loss :  0.0754388058216784  val loss :  0.12032344922174683\n",
      "epoch :  8 train loss :  0.07562546640712553  val loss :  0.11749714814256756\n",
      "epoch :  9 train loss :  0.07525337590831112  val loss :  0.11542104746064916\n",
      "epoch :  10 train loss :  0.0739053043278412  val loss :  0.11389263893186868\n",
      "epoch :  11 train loss :  0.0728394078256572  val loss :  0.11274276016446141\n",
      "epoch :  12 train loss :  0.07014883676280526  val loss :  0.1118735031400725\n",
      "epoch :  13 train loss :  0.06799341833794305  val loss :  0.11117941013590368\n",
      "epoch :  14 train loss :  0.06594950142097099  val loss :  0.1105497186263795\n",
      "epoch :  15 train loss :  0.06390899045305103  val loss :  0.10981559449398204\n",
      "epoch :  16 train loss :  0.06235870026559106  val loss :  0.10882343184205791\n",
      "epoch :  17 train loss :  0.06072705521867537  val loss :  0.10738660387422816\n",
      "epoch :  18 train loss :  0.059383563775354656  val loss :  0.10541208446029306\n",
      "epoch :  19 train loss :  0.05715366041395053  val loss :  0.10285201036566662\n",
      "epoch :  20 train loss :  0.054436339270225995  val loss :  0.09965598951665844\n",
      "epoch :  21 train loss :  0.0526192139892678  val loss :  0.09587729491577553\n",
      "epoch :  22 train loss :  0.05028644382602569  val loss :  0.09166055590395615\n",
      "epoch :  23 train loss :  0.04741718504987462  val loss :  0.08718623023046032\n",
      "epoch :  24 train loss :  0.04664130485494723  val loss :  0.08265087838589182\n",
      "epoch :  25 train loss :  0.04452323805326255  val loss :  0.07833797470140222\n",
      "epoch :  26 train loss :  0.04380501682884718  val loss :  0.0746114271864601\n",
      "epoch :  27 train loss :  0.04426312685519925  val loss :  0.07177474638376567\n",
      "epoch :  28 train loss :  0.04520477658791068  val loss :  0.07000911948219228\n",
      "epoch :  29 train loss :  0.04582273972791215  val loss :  0.06918681338417924\n",
      "epoch :  30 train loss :  0.04805714503437749  val loss :  0.06865500409394101\n",
      "epoch :  31 train loss :  0.05090752448789112  val loss :  0.06751978118275177\n",
      "epoch :  32 train loss :  0.05225091591399378  val loss :  0.0660090642453328\n",
      "epoch :  33 train loss :  0.05548229175790442  val loss :  0.06538404834382974\n",
      "epoch :  34 train loss :  0.055554600403258936  val loss :  0.06604600694470827\n",
      "epoch :  35 train loss :  0.05557986982751891  val loss :  0.067817781748\n",
      "epoch :  36 train loss :  0.054878274222631106  val loss :  0.06865386783919357\n",
      "epoch :  37 train loss :  0.05148350477647719  val loss :  0.06803592509304238\n",
      "epoch :  38 train loss :  0.04910189931461324  val loss :  0.06797184739728679\n",
      "epoch :  39 train loss :  0.04602729300943969  val loss :  0.0694388894794744\n",
      "epoch :  40 train loss :  0.04567519397869784  val loss :  0.07234722818247974\n",
      "epoch :  41 train loss :  0.04432584405803556  val loss :  0.07566015709939039\n",
      "epoch :  42 train loss :  0.04556360550185773  val loss :  0.07811712219202914\n",
      "epoch :  43 train loss :  0.04321870583241211  val loss :  0.07963589434248296\n",
      "epoch :  44 train loss :  0.043677920241552495  val loss :  0.08114025028307068\n",
      "epoch :  45 train loss :  0.0453589385014554  val loss :  0.08311902010216114\n",
      "epoch :  46 train loss :  0.046036115754883325  val loss :  0.08564702990052767\n",
      "epoch :  47 train loss :  0.04697759458258826  val loss :  0.08846604849722918\n",
      "epoch :  48 train loss :  0.04809660775616219  val loss :  0.09088057964398481\n",
      "epoch :  49 train loss :  0.0487617181994372  val loss :  0.09234152174470335\n",
      "epoch :  50 train loss :  0.049503339308250634  val loss :  0.09282069708454643\n",
      "epoch :  51 train loss :  0.05041859249208922  val loss :  0.09257441412284048\n",
      "epoch :  52 train loss :  0.04999824283712821  val loss :  0.09207402056660549\n",
      "epoch :  53 train loss :  0.0503653985355537  val loss :  0.0916646287769614\n",
      "epoch :  54 train loss :  0.0499946506235612  val loss :  0.0913571466404788\n",
      "epoch :  55 train loss :  0.049323421907830615  val loss :  0.09089964936127685\n",
      "epoch :  56 train loss :  0.04787186739956521  val loss :  0.08985214276635635\n",
      "epoch :  57 train loss :  0.04655571947001038  val loss :  0.08790684644960078\n",
      "epoch :  58 train loss :  0.046703845125998504  val loss :  0.08533534133747897\n",
      "epoch :  59 train loss :  0.04524435712443908  val loss :  0.08271548290614723\n",
      "epoch :  60 train loss :  0.044630778787647866  val loss :  0.08055352126561191\n",
      "epoch :  61 train loss :  0.04410232733252473  val loss :  0.0789176975393473\n",
      "epoch :  62 train loss :  0.043849684034732626  val loss :  0.07724935269069115\n",
      "epoch :  63 train loss :  0.042954355780600875  val loss :  0.07532528851433118\n",
      "epoch :  64 train loss :  0.044334387055632335  val loss :  0.07321015127996758\n",
      "epoch :  65 train loss :  0.04439307157320814  val loss :  0.07158380392783248\n",
      "epoch :  66 train loss :  0.045032859657993494  val loss :  0.07097048598766963\n",
      "epoch :  67 train loss :  0.0468588015893523  val loss :  0.07067209282241033\n",
      "epoch :  68 train loss :  0.04699277702270378  val loss :  0.07003391327741003\n",
      "epoch :  69 train loss :  0.04675873833176977  val loss :  0.06957229135078516\n",
      "epoch :  70 train loss :  0.04606398108234892  val loss :  0.06995295369357912\n",
      "epoch :  71 train loss :  0.04573859601081666  val loss :  0.07076429998811039\n",
      "epoch :  72 train loss :  0.04490987939945378  val loss :  0.0714635476740861\n",
      "epoch :  73 train loss :  0.04554328306807273  val loss :  0.07207581203080493\n",
      "epoch :  74 train loss :  0.04612212799762556  val loss :  0.07305426160415339\n",
      "epoch :  75 train loss :  0.04479005502741686  val loss :  0.07423671093160784\n",
      "epoch :  76 train loss :  0.043559358411356414  val loss :  0.07576395675299648\n",
      "epoch :  77 train loss :  0.04435900069227081  val loss :  0.07710091212861203\n",
      "epoch :  78 train loss :  0.043584815202115094  val loss :  0.0779894210357423\n",
      "epoch :  79 train loss :  0.04412738663870939  val loss :  0.07864257141792934\n",
      "epoch :  80 train loss :  0.04467145608084676  val loss :  0.07951074904348848\n",
      "epoch :  81 train loss :  0.0446073675713458  val loss :  0.08036507811529289\n",
      "epoch :  82 train loss :  0.04431190572835076  val loss :  0.0807228031211131\n",
      "epoch :  83 train loss :  0.0444769486784935  val loss :  0.08041801259918033\n",
      "epoch :  84 train loss :  0.04423811843530982  val loss :  0.0798811399617629\n",
      "epoch :  85 train loss :  0.044848494387734  val loss :  0.07954822142883221\n",
      "epoch :  86 train loss :  0.044752611344709446  val loss :  0.07923933607684609\n",
      "epoch :  87 train loss :  0.04338339213964515  val loss :  0.07865059219646824\n",
      "epoch :  88 train loss :  0.044419612452699875  val loss :  0.0772726589673064\n",
      "epoch :  89 train loss :  0.044456850161961235  val loss :  0.07590699749893572\n",
      "epoch :  90 train loss :  0.04423005578561603  val loss :  0.07522492783488752\n",
      "epoch :  91 train loss :  0.042760136600642304  val loss :  0.07510681692519489\n",
      "epoch :  92 train loss :  0.04391027149300613  val loss :  0.07453295212218279\n",
      "epoch :  93 train loss :  0.04473109696430998  val loss :  0.07349955615006032\n",
      "epoch :  94 train loss :  0.043597752023585803  val loss :  0.07320121565855588\n",
      "epoch :  95 train loss :  0.0437432459375153  val loss :  0.0738261539075814\n",
      "epoch :  96 train loss :  0.04351278764064087  val loss :  0.07437041882638207\n",
      "epoch :  97 train loss :  0.04426351884935851  val loss :  0.07447596208571622\n",
      "epoch :  98 train loss :  0.043345807919873615  val loss :  0.07468962832339601\n",
      "epoch :  99 train loss :  0.043062413901246654  val loss :  0.07537073655097773\n",
      "epoch :  100 train loss :  0.04334187036604469  val loss :  0.07640121435492396\n",
      "epoch :  101 train loss :  0.0433445618980374  val loss :  0.0770995481307722\n",
      "epoch :  102 train loss :  0.04360820115080679  val loss :  0.0770577595496866\n",
      "epoch :  103 train loss :  0.043989169474749665  val loss :  0.07731658105619164\n",
      "epoch :  104 train loss :  0.0436885393278293  val loss :  0.0778788085846809\n",
      "epoch :  105 train loss :  0.04296665856156362  val loss :  0.0779000329378748\n",
      "epoch :  106 train loss :  0.04359508684168311  val loss :  0.07734794958212499\n",
      "epoch :  107 train loss :  0.04424099618305711  val loss :  0.07655602816694317\n",
      "epoch :  108 train loss :  0.043654671055874276  val loss :  0.07636386098321135\n",
      "epoch :  109 train loss :  0.0447766856139243  val loss :  0.07605601692835035\n",
      "epoch :  110 train loss :  0.04453591686424785  val loss :  0.07534448284523834\n",
      "epoch :  111 train loss :  0.04360979528249246  val loss :  0.07509734385616912\n",
      "epoch :  112 train loss :  0.04308528404351304  val loss :  0.07513349043884854\n",
      "epoch :  113 train loss :  0.04310303741178587  val loss :  0.07521382774042523\n",
      "epoch :  114 train loss :  0.04332117079050129  val loss :  0.07500298220910925\n",
      "epoch :  115 train loss :  0.04365708098712704  val loss :  0.07530217332547037\n",
      "epoch :  116 train loss :  0.043892716267268074  val loss :  0.07600053496232906\n",
      "epoch :  117 train loss :  0.043959273698286234  val loss :  0.07606462763589553\n",
      "epoch :  118 train loss :  0.04340923181613078  val loss :  0.07601151264716104\n",
      "epoch :  119 train loss :  0.044524832214204425  val loss :  0.0762910372798768\n",
      "epoch :  120 train loss :  0.04350760863869602  val loss :  0.07655822045569156\n",
      "epoch :  121 train loss :  0.04369781727059037  val loss :  0.07624181457374495\n",
      "epoch :  122 train loss :  0.043077003981196446  val loss :  0.07585576249842386\n",
      "epoch :  123 train loss :  0.04223016231120881  val loss :  0.07623843121599397\n",
      "epoch :  124 train loss :  0.04374361948076031  val loss :  0.07572586946869453\n",
      "epoch :  125 train loss :  0.043877102902734465  val loss :  0.07490608062963106\n",
      "epoch :  126 train loss :  0.04368342914623428  val loss :  0.07548323822121879\n",
      "epoch :  127 train loss :  0.04256918204275413  val loss :  0.07580063467436715\n",
      "epoch :  128 train loss :  0.04477205379081022  val loss :  0.07512838033016132\n",
      "epoch :  129 train loss :  0.04261060947484059  val loss :  0.07619283149652252\n",
      "epoch :  130 train loss :  0.0431507837553923  val loss :  0.07682184566381171\n",
      "epoch :  131 train loss :  0.04303784533867037  val loss :  0.07551526314042835\n",
      "epoch :  132 train loss :  0.043220444197195985  val loss :  0.07625065900723675\n",
      "epoch :  133 train loss :  0.043031388571905214  val loss :  0.0767396410598966\n",
      "epoch :  134 train loss :  0.0439473710056999  val loss :  0.07500593498260437\n",
      "epoch :  135 train loss :  0.04341882369749209  val loss :  0.0757266340645014\n",
      "epoch :  136 train loss :  0.04370850184200946  val loss :  0.07673586885753723\n",
      "epoch :  137 train loss :  0.04364813679917008  val loss :  0.07500027535397659\n",
      "epoch :  138 train loss :  0.043069024321608515  val loss :  0.0756878113052243\n",
      "epoch :  139 train loss :  0.04399386130234334  val loss :  0.07684698695314605\n",
      "epoch :  140 train loss :  0.043483212356167936  val loss :  0.07468671222984022\n",
      "epoch :  141 train loss :  0.043838630669326056  val loss :  0.07528924290595074\n",
      "epoch :  142 train loss :  0.04378533268747217  val loss :  0.07655129843840426\n",
      "epoch :  143 train loss :  0.04430995862053327  val loss :  0.07435492049939561\n",
      "epoch :  144 train loss :  0.04471514621720264  val loss :  0.07534609148137636\n",
      "epoch :  145 train loss :  0.042962700391861154  val loss :  0.07711628868502776\n",
      "epoch :  146 train loss :  0.04379547169890391  val loss :  0.07469920160229497\n",
      "epoch :  147 train loss :  0.04365860249515603  val loss :  0.0760980222087773\n",
      "epoch :  148 train loss :  0.044468346173772634  val loss :  0.07738294656731945\n",
      "epoch :  149 train loss :  0.042785789329968196  val loss :  0.07464089783244694\n",
      "epoch :  150 train loss :  0.04387799478794268  val loss :  0.07653075816599067\n",
      "epoch :  151 train loss :  0.04378843022734707  val loss :  0.07689566294086918\n",
      "epoch :  152 train loss :  0.043111228388953586  val loss :  0.07394413997707767\n",
      "epoch :  153 train loss :  0.04424340715778124  val loss :  0.07680093975968792\n",
      "epoch :  154 train loss :  0.04392780219229104  val loss :  0.07607004150623192\n",
      "epoch :  155 train loss :  0.045045183903259754  val loss :  0.07435057354236375\n",
      "epoch :  156 train loss :  0.0438336334654486  val loss :  0.07894101713456693\n",
      "epoch :  157 train loss :  0.044191581143446616  val loss :  0.07607335712690665\n",
      "epoch :  158 train loss :  0.04337334488542916  val loss :  0.07460188338664371\n",
      "epoch :  159 train loss :  0.044565031968096164  val loss :  0.07938774288366872\n",
      "epoch :  160 train loss :  0.04413503490349385  val loss :  0.0743398334800959\n",
      "epoch :  161 train loss :  0.044453258962609384  val loss :  0.07375169317442264\n",
      "epoch :  162 train loss :  0.04439751461577353  val loss :  0.07980291208507041\n",
      "epoch :  163 train loss :  0.04467670248909146  val loss :  0.07407677835505125\n",
      "epoch :  164 train loss :  0.04479838573222697  val loss :  0.073977673480516\n",
      "epoch :  165 train loss :  0.04480045411677261  val loss :  0.08127837759929037\n",
      "epoch :  166 train loss :  0.044633462147406884  val loss :  0.07540715635605912\n",
      "epoch :  167 train loss :  0.045470501125561  val loss :  0.0738790053114601\n",
      "epoch :  168 train loss :  0.04444353392650008  val loss :  0.08026358482340296\n",
      "epoch :  169 train loss :  0.0454118145405466  val loss :  0.07816948211523658\n",
      "epoch :  170 train loss :  0.0439860847588453  val loss :  0.07320130878692323\n",
      "epoch :  171 train loss :  0.044710725750676625  val loss :  0.07532226246502711\n",
      "epoch :  172 train loss :  0.04415582124319376  val loss :  0.0812592202394689\n",
      "epoch :  173 train loss :  0.04518871662738436  val loss :  0.07405733061959566\n",
      "epoch :  174 train loss :  0.045325796651356505  val loss :  0.07281180155109948\n",
      "epoch :  175 train loss :  0.04406988945501949  val loss :  0.07763731263152619\n",
      "epoch :  176 train loss :  0.045903022698472934  val loss :  0.08264742468758647\n",
      "epoch :  177 train loss :  0.04474003031771845  val loss :  0.07590920868669584\n",
      "epoch :  178 train loss :  0.045468633272807014  val loss :  0.07380817303970562\n",
      "epoch :  179 train loss :  0.045376423141719156  val loss :  0.0758074973337799\n",
      "epoch :  180 train loss :  0.04449107635434697  val loss :  0.08149370756968487\n",
      "epoch :  181 train loss :  0.04586260473228874  val loss :  0.07834857101102664\n",
      "epoch :  182 train loss :  0.042161723228259236  val loss :  0.0727911969351955\n",
      "epoch :  183 train loss :  0.045039224384732894  val loss :  0.0723325552034571\n",
      "epoch :  184 train loss :  0.04404610150654591  val loss :  0.07579682542816252\n",
      "epoch :  185 train loss :  0.04449182967240898  val loss :  0.0811756858337062\n",
      "epoch :  186 train loss :  0.04462027570222997  val loss :  0.08035370355096212\n",
      "epoch :  187 train loss :  0.043416913842577586  val loss :  0.07535941700574551\n",
      "epoch :  188 train loss :  0.044071866989291775  val loss :  0.07325379779593835\n",
      "epoch :  189 train loss :  0.044377936313601686  val loss :  0.07439390476666081\n",
      "epoch :  190 train loss :  0.04365733562339663  val loss :  0.07820838529617746\n",
      "epoch :  191 train loss :  0.04383838095785123  val loss :  0.08073519326061804\n",
      "epoch :  192 train loss :  0.044639629150950474  val loss :  0.07814501614873481\n",
      "epoch :  193 train loss :  0.044274895544607604  val loss :  0.07376093840016754\n",
      "epoch :  194 train loss :  0.043920194856939515  val loss :  0.07231231863504332\n",
      "epoch :  195 train loss :  0.04442352986850664  val loss :  0.0741648006228163\n",
      "epoch :  196 train loss :  0.04439200200530559  val loss :  0.07862789044416012\n",
      "epoch :  197 train loss :  0.04472459999951705  val loss :  0.08106961381612438\n",
      "epoch :  198 train loss :  0.044556567055314626  val loss :  0.07830275202718665\n",
      "epoch :  199 train loss :  0.042832606164884816  val loss :  0.07446345773410885\n",
      "epoch :  200 train loss :  0.04536231343892856  val loss :  0.07334996222129805\n",
      "epoch :  201 train loss :  0.04377455061486878  val loss :  0.0753702550425827\n",
      "epoch :  202 train loss :  0.0435814600223334  val loss :  0.07907924196558387\n",
      "epoch :  203 train loss :  0.04482639954469279  val loss :  0.07856171082251125\n",
      "epoch :  204 train loss :  0.04388495790162636  val loss :  0.07489995412128121\n",
      "epoch :  205 train loss :  0.0442454355003322  val loss :  0.07362162323705987\n",
      "epoch :  206 train loss :  0.04503278348458375  val loss :  0.07534203907663042\n",
      "epoch :  207 train loss :  0.04361922027669964  val loss :  0.07899649108161845\n",
      "epoch :  208 train loss :  0.04396335620686646  val loss :  0.07820694089381919\n",
      "epoch :  209 train loss :  0.04520484651999636  val loss :  0.07416446632188388\n",
      "epoch :  210 train loss :  0.044773444835662216  val loss :  0.07316550237642377\n",
      "epoch :  211 train loss :  0.044299638405910335  val loss :  0.07595473608505765\n",
      "epoch :  212 train loss :  0.043373152662400175  val loss :  0.07954342478349123\n",
      "epoch :  213 train loss :  0.044123636032040205  val loss :  0.07681157481384067\n",
      "epoch :  214 train loss :  0.04454974815522501  val loss :  0.07416424900675157\n",
      "epoch :  215 train loss :  0.04486021957316324  val loss :  0.07503884916712034\n",
      "epoch :  216 train loss :  0.044060403929721  val loss :  0.07881636657443193\n",
      "epoch :  217 train loss :  0.044587880021926624  val loss :  0.07681165729841853\n",
      "epoch :  218 train loss :  0.04463755039995566  val loss :  0.07332919546598253\n",
      "epoch :  219 train loss :  0.045611868363790484  val loss :  0.07433897181234365\n",
      "epoch :  220 train loss :  0.044811411034218304  val loss :  0.07946425844744623\n",
      "epoch :  221 train loss :  0.04460243670104062  val loss :  0.07923604622523596\n",
      "epoch :  222 train loss :  0.043690860700544884  val loss :  0.07452300063192921\n",
      "epoch :  223 train loss :  0.0432862089800585  val loss :  0.07329728149785963\n",
      "epoch :  224 train loss :  0.044357146531421476  val loss :  0.07608798311159964\n",
      "epoch :  225 train loss :  0.045456299675306726  val loss :  0.0789368791997866\n",
      "epoch :  226 train loss :  0.044661899950569836  val loss :  0.07675957737581593\n",
      "epoch :  227 train loss :  0.0443384538048693  val loss :  0.07542334083600598\n",
      "epoch :  228 train loss :  0.04476027453015924  val loss :  0.07557478504175306\n",
      "epoch :  229 train loss :  0.04345725453336825  val loss :  0.07790455626528987\n",
      "epoch :  230 train loss :  0.04415058802949821  val loss :  0.07809777044815629\n",
      "epoch :  231 train loss :  0.04321072795745278  val loss :  0.07536626915435311\n",
      "epoch :  232 train loss :  0.04431315513415486  val loss :  0.07477885256430274\n",
      "epoch :  233 train loss :  0.04457893515132485  val loss :  0.07636857568610536\n",
      "epoch :  234 train loss :  0.04371921825393332  val loss :  0.0790909295184146\n",
      "epoch :  235 train loss :  0.04371545337023535  val loss :  0.07945208763761336\n",
      "epoch :  236 train loss :  0.044548849667866194  val loss :  0.0761596128681103\n",
      "epoch :  237 train loss :  0.04408599424580629  val loss :  0.07382670781694152\n",
      "epoch :  238 train loss :  0.04391965323678798  val loss :  0.0749056393610997\n",
      "epoch :  239 train loss :  0.04283699596115432  val loss :  0.07835027156657041\n",
      "epoch :  240 train loss :  0.042836862260290466  val loss :  0.08018229869805221\n",
      "epoch :  241 train loss :  0.044166262968594494  val loss :  0.07689942913256714\n",
      "epoch :  242 train loss :  0.044001890492688926  val loss :  0.07311035802573171\n",
      "epoch :  243 train loss :  0.04349769612881526  val loss :  0.07420717579787217\n",
      "epoch :  244 train loss :  0.04383322195739958  val loss :  0.07955876161087314\n",
      "epoch :  245 train loss :  0.042590574054193746  val loss :  0.07970111745341663\n",
      "epoch :  246 train loss :  0.04250741238018293  val loss :  0.07350402240486387\n",
      "epoch :  247 train loss :  0.042768748535613736  val loss :  0.07222543828401745\n",
      "epoch :  248 train loss :  0.043523627455243886  val loss :  0.07737359449775696\n",
      "epoch :  249 train loss :  0.04347336355655293  val loss :  0.0776660865893712\n",
      "epoch :  250 train loss :  0.04370099304156154  val loss :  0.07522943589096805\n",
      "epoch :  251 train loss :  0.043798719264605906  val loss :  0.07760684657669027\n",
      "epoch :  252 train loss :  0.04316341253826443  val loss :  0.07554428987631501\n",
      "epoch :  253 train loss :  0.043410000522957425  val loss :  0.07091696711510245\n",
      "epoch :  254 train loss :  0.04383895193172999  val loss :  0.07878871292128264\n",
      "epoch :  255 train loss :  0.044516503430551885  val loss :  0.07911148452370584\n",
      "epoch :  256 train loss :  0.044282361845539506  val loss :  0.07221303976613926\n",
      "epoch :  257 train loss :  0.043694874622824925  val loss :  0.07946888827038745\n",
      "epoch :  258 train loss :  0.04406079413925166  val loss :  0.074535665196026\n",
      "epoch :  259 train loss :  0.04442091563141159  val loss :  0.07381755429320613\n",
      "epoch :  260 train loss :  0.04453323281209194  val loss :  0.08555546551504827\n",
      "epoch :  261 train loss :  0.04429371190788858  val loss :  0.07160278632619509\n",
      "epoch :  262 train loss :  0.0465737863185362  val loss :  0.07316862238832694\n",
      "epoch :  263 train loss :  0.046149570631419175  val loss :  0.0875749376580571\n",
      "epoch :  264 train loss :  0.04391577556066176  val loss :  0.07312356232382221\n",
      "epoch :  265 train loss :  0.04735577267384966  val loss :  0.07316650619651398\n",
      "epoch :  266 train loss :  0.04576164237296706  val loss :  0.08666688526374297\n",
      "epoch :  267 train loss :  0.04394105675566883  val loss :  0.07406675239589375\n",
      "epoch :  268 train loss :  0.047752182472360696  val loss :  0.07222064336995815\n",
      "epoch :  269 train loss :  0.044096003798524123  val loss :  0.08159419816416126\n",
      "epoch :  270 train loss :  0.04819447227329484  val loss :  0.08204355628153605\n",
      "epoch :  271 train loss :  0.043545311004781595  val loss :  0.0727609419443277\n",
      "epoch :  272 train loss :  0.04784388876281172  val loss :  0.07366281251006962\n",
      "epoch :  273 train loss :  0.04366673401396  val loss :  0.07885292057399776\n",
      "epoch :  274 train loss :  0.046466304369621875  val loss :  0.08354698991939531\n",
      "epoch :  275 train loss :  0.0468699354190789  val loss :  0.07711883156968391\n",
      "epoch :  276 train loss :  0.04306829228256073  val loss :  0.07322155469186742\n",
      "epoch :  277 train loss :  0.046443905583851  val loss :  0.07587520555750983\n",
      "epoch :  278 train loss :  0.04511271195768998  val loss :  0.07782732581762332\n",
      "epoch :  279 train loss :  0.04363022651035748  val loss :  0.0784474331574367\n",
      "epoch :  280 train loss :  0.04390826197194803  val loss :  0.07791977338170135\n",
      "epoch :  281 train loss :  0.04326850051031063  val loss :  0.0769927796462809\n",
      "epoch :  282 train loss :  0.04462723297442441  val loss :  0.07665415546605547\n",
      "epoch :  283 train loss :  0.04316007528278528  val loss :  0.07726594761460195\n",
      "epoch :  284 train loss :  0.044229782990994254  val loss :  0.07699474159944476\n",
      "epoch :  285 train loss :  0.045453722730122936  val loss :  0.07662881246565437\n",
      "epoch :  286 train loss :  0.04442526257709059  val loss :  0.0782663937997232\n",
      "epoch :  287 train loss :  0.04321155963338794  val loss :  0.07746938647849873\n",
      "epoch :  288 train loss :  0.04485204215137122  val loss :  0.07516688634733679\n",
      "epoch :  289 train loss :  0.04474279859849296  val loss :  0.07550162214343206\n",
      "epoch :  290 train loss :  0.04350201470572599  val loss :  0.07759692811906813\n",
      "epoch :  291 train loss :  0.04401129275482363  val loss :  0.07770407165479819\n",
      "epoch :  292 train loss :  0.045034377362715636  val loss :  0.07695301510971554\n",
      "epoch :  293 train loss :  0.044337460907732  val loss :  0.07759965750715715\n",
      "epoch :  294 train loss :  0.04352727850304224  val loss :  0.07761575369795987\n",
      "epoch :  295 train loss :  0.043343304329751677  val loss :  0.07422181555458418\n",
      "epoch :  296 train loss :  0.043984296756031  val loss :  0.07348744693877277\n",
      "epoch :  297 train loss :  0.04435153743398439  val loss :  0.07670528668346054\n",
      "epoch :  298 train loss :  0.043181412039002824  val loss :  0.07852626217726817\n",
      "epoch :  299 train loss :  0.043539603791779874  val loss :  0.07716750756846505\n",
      "epoch :  300 train loss :  0.043464421182480784  val loss :  0.07579403221987575\n",
      "epoch :  301 train loss :  0.04425868266188974  val loss :  0.07427653987326609\n",
      "epoch :  302 train loss :  0.04350063069211563  val loss :  0.0737785563794412\n",
      "epoch :  303 train loss :  0.04385668740495649  val loss :  0.07425251851647303\n",
      "epoch :  304 train loss :  0.04310892423839157  val loss :  0.07715948475440497\n",
      "epoch :  305 train loss :  0.04360309509433689  val loss :  0.07828009481498625\n",
      "epoch :  306 train loss :  0.044157038369649994  val loss :  0.07642929837487882\n",
      "epoch :  307 train loss :  0.043733221337355244  val loss :  0.07574304930665784\n",
      "epoch :  308 train loss :  0.04414869885872172  val loss :  0.07486385211987089\n",
      "epoch :  309 train loss :  0.04428312952839891  val loss :  0.07410307737896657\n",
      "epoch :  310 train loss :  0.04367506434819149  val loss :  0.0761912769132575\n",
      "epoch :  311 train loss :  0.043118045211852536  val loss :  0.07701796646740537\n",
      "epoch :  312 train loss :  0.04408370561078581  val loss :  0.07612438504862205\n",
      "epoch :  313 train loss :  0.04423805635363956  val loss :  0.07695500302282081\n",
      "epoch :  314 train loss :  0.04347123632802389  val loss :  0.0772304055631094\n",
      "epoch :  315 train loss :  0.04375993720797032  val loss :  0.07410578584165363\n",
      "epoch :  316 train loss :  0.04302509102716808  val loss :  0.07415851990151735\n",
      "epoch :  317 train loss :  0.042596874524551534  val loss :  0.07649352371440232\n",
      "epoch :  318 train loss :  0.04345442009455871  val loss :  0.07560903496799949\n",
      "epoch :  319 train loss :  0.04404895650271658  val loss :  0.07511436016280677\n",
      "epoch :  320 train loss :  0.042386820705617285  val loss :  0.07737743159344576\n",
      "epoch :  321 train loss :  0.04241343772926256  val loss :  0.07675293486091045\n",
      "epoch :  322 train loss :  0.04339656179175951  val loss :  0.07456270993501639\n",
      "epoch :  323 train loss :  0.04300812531399165  val loss :  0.0758952052595548\n",
      "epoch :  324 train loss :  0.04293820596451223  val loss :  0.07671094226643978\n",
      "epoch :  325 train loss :  0.043023975244211275  val loss :  0.07505252059087467\n",
      "epoch :  326 train loss :  0.0432485096566184  val loss :  0.0757516335271239\n",
      "epoch :  327 train loss :  0.0431153468339075  val loss :  0.07684430491803375\n",
      "epoch :  328 train loss :  0.042605168200756244  val loss :  0.07605406853618471\n",
      "epoch :  329 train loss :  0.04282370573446077  val loss :  0.07593547647512325\n",
      "epoch :  330 train loss :  0.04254547961992431  val loss :  0.07657525913027982\n",
      "epoch :  331 train loss :  0.04370658453651436  val loss :  0.07596631578384729\n",
      "epoch :  332 train loss :  0.04265648389908032  val loss :  0.07430534959020955\n",
      "epoch :  333 train loss :  0.042939474835445746  val loss :  0.0759578858632456\n",
      "epoch :  334 train loss :  0.042884259411567795  val loss :  0.07688563252484545\n",
      "epoch :  335 train loss :  0.0440162807820043  val loss :  0.07453777901292317\n",
      "epoch :  336 train loss :  0.042786846434726765  val loss :  0.0779616127169857\n",
      "epoch :  337 train loss :  0.04345461688155591  val loss :  0.07840114617867207\n",
      "epoch :  338 train loss :  0.0438943024315135  val loss :  0.07376583307920402\n",
      "epoch :  339 train loss :  0.042940147903733225  val loss :  0.07694780838411845\n",
      "epoch :  340 train loss :  0.04252410182700107  val loss :  0.07738746545510367\n",
      "epoch :  341 train loss :  0.042838283497547605  val loss :  0.07227519646158446\n",
      "epoch :  342 train loss :  0.04401518194814315  val loss :  0.07793278762284611\n",
      "epoch :  343 train loss :  0.04353119515818763  val loss :  0.07739934266837646\n",
      "epoch :  344 train loss :  0.042832315875051534  val loss :  0.07264218396041974\n",
      "epoch :  345 train loss :  0.043917353523417295  val loss :  0.07953598274443631\n",
      "epoch :  346 train loss :  0.042597478158108854  val loss :  0.07695248995957744\n",
      "epoch :  347 train loss :  0.043862240788824274  val loss :  0.07166697652385187\n",
      "epoch :  348 train loss :  0.04290900450609429  val loss :  0.07883359167850484\n",
      "epoch :  349 train loss :  0.04271502182355726  val loss :  0.07678945402247084\n",
      "epoch :  350 train loss :  0.043719841742702804  val loss :  0.07200392148599961\n",
      "epoch :  351 train loss :  0.04423232773484672  val loss :  0.07984370817727118\n",
      "epoch :  352 train loss :  0.044633743348312  val loss :  0.07919694569928913\n",
      "epoch :  353 train loss :  0.04382890880263913  val loss :  0.07059101476877211\n",
      "epoch :  354 train loss :  0.04345264889455903  val loss :  0.07636530577126743\n",
      "epoch :  355 train loss :  0.04324743710458279  val loss :  0.08083821666838445\n",
      "epoch :  356 train loss :  0.042962777277171925  val loss :  0.07150797474338952\n",
      "epoch :  357 train loss :  0.04421170804825129  val loss :  0.07409395224659411\n",
      "epoch :  358 train loss :  0.04402217281621476  val loss :  0.08304315192900405\n",
      "epoch :  359 train loss :  0.04364235581176755  val loss :  0.07431128554652436\n",
      "epoch :  360 train loss :  0.04324156765616377  val loss :  0.07023929322707827\n",
      "epoch :  361 train loss :  0.04312517775798031  val loss :  0.07938637036099998\n",
      "epoch :  362 train loss :  0.04346383699454874  val loss :  0.08136646117378801\n",
      "epoch :  363 train loss :  0.04308520366696163  val loss :  0.07200843189415951\n",
      "epoch :  364 train loss :  0.04329274239808477  val loss :  0.0705341647472319\n",
      "epoch :  365 train loss :  0.04354099352010258  val loss :  0.07943290601211304\n",
      "epoch :  366 train loss :  0.044125042514417186  val loss :  0.0823765568245585\n",
      "epoch :  367 train loss :  0.04282255067992273  val loss :  0.07395464596903358\n",
      "epoch :  368 train loss :  0.044037371639805936  val loss :  0.0701528264194932\n",
      "epoch :  369 train loss :  0.04365485877344746  val loss :  0.07548872806724867\n",
      "epoch :  370 train loss :  0.044014699659578466  val loss :  0.08188748124371384\n",
      "epoch :  371 train loss :  0.04387233781759964  val loss :  0.07741208204804773\n",
      "epoch :  372 train loss :  0.04286584677418489  val loss :  0.07092792258856098\n",
      "epoch :  373 train loss :  0.04487330364112142  val loss :  0.0709364614739039\n",
      "epoch :  374 train loss :  0.04419151812557775  val loss :  0.07822889567261773\n",
      "epoch :  375 train loss :  0.04617727379173196  val loss :  0.0810085237607751\n",
      "epoch :  376 train loss :  0.044520195710097306  val loss :  0.08006692092317055\n",
      "epoch :  377 train loss :  0.04358732349273422  val loss :  0.0711796684426199\n",
      "epoch :  378 train loss :  0.044742267305782325  val loss :  0.0713471611324896\n",
      "epoch :  379 train loss :  0.04580181964834011  val loss :  0.07706152446600285\n",
      "epoch :  380 train loss :  0.0440804345133417  val loss :  0.08037022500884451\n",
      "epoch :  381 train loss :  0.04695135148251868  val loss :  0.08062978994971808\n",
      "epoch :  382 train loss :  0.0454144610959978  val loss :  0.07613353350310412\n",
      "epoch :  383 train loss :  0.045565127302448785  val loss :  0.06831070266444741\n",
      "epoch :  384 train loss :  0.04548118645764146  val loss :  0.07281883706803068\n",
      "epoch :  385 train loss :  0.04438907947605817  val loss :  0.08619455755561461\n",
      "epoch :  386 train loss :  0.04616569079146647  val loss :  0.08783535330641627\n",
      "epoch :  387 train loss :  0.04412439098844977  val loss :  0.07198471504577382\n",
      "epoch :  388 train loss :  0.04514188871411753  val loss :  0.06689170463836815\n",
      "epoch :  389 train loss :  0.04482896592838602  val loss :  0.07180251493257019\n",
      "epoch :  390 train loss :  0.04403941659988221  val loss :  0.0837405299122957\n",
      "epoch :  391 train loss :  0.04485114508781446  val loss :  0.08390861652626674\n",
      "epoch :  392 train loss :  0.0438083443615137  val loss :  0.07633650331135328\n",
      "epoch :  393 train loss :  0.045407902215319776  val loss :  0.07283587756964072\n",
      "epoch :  394 train loss :  0.04346673690874851  val loss :  0.07489436982536117\n",
      "epoch :  395 train loss :  0.043307294259676755  val loss :  0.07941289138455765\n",
      "epoch :  396 train loss :  0.0450496470545911  val loss :  0.07768079089139136\n",
      "epoch :  397 train loss :  0.043034672005716416  val loss :  0.07208965331117742\n",
      "epoch :  398 train loss :  0.042769728628284646  val loss :  0.07148430528558501\n",
      "epoch :  399 train loss :  0.04509986906814638  val loss :  0.07689892468693185\n",
      "epoch :  400 train loss :  0.0438054076137499  val loss :  0.0843850019616121\n",
      "epoch :  401 train loss :  0.04406209075482104  val loss :  0.0845715580920177\n",
      "epoch :  402 train loss :  0.04318704537033099  val loss :  0.07687102628400375\n",
      "epoch :  403 train loss :  0.043053582505717954  val loss :  0.07076065321432745\n",
      "epoch :  404 train loss :  0.04464925999182681  val loss :  0.06992108602686797\n",
      "epoch :  405 train loss :  0.04379622206208906  val loss :  0.07357127081325028\n",
      "epoch :  406 train loss :  0.043818657603407404  val loss :  0.07777494111583605\n",
      "epoch :  407 train loss :  0.043023194990935126  val loss :  0.07833919000505277\n",
      "epoch :  408 train loss :  0.04337986674725385  val loss :  0.07613054763216028\n",
      "epoch :  409 train loss :  0.04350878796613341  val loss :  0.07456298565743018\n",
      "epoch :  410 train loss :  0.04320726922045203  val loss :  0.07442596142537146\n",
      "epoch :  411 train loss :  0.04374030200247677  val loss :  0.0756145815822033\n",
      "epoch :  412 train loss :  0.043916087353806844  val loss :  0.07626992670904457\n",
      "epoch :  413 train loss :  0.04378782142519327  val loss :  0.07486578222366493\n",
      "epoch :  414 train loss :  0.04279722231415866  val loss :  0.0737395012099411\n",
      "epoch :  415 train loss :  0.04228411537853523  val loss :  0.07427738370720498\n",
      "epoch :  416 train loss :  0.04228613005843774  val loss :  0.07649298523628573\n",
      "epoch :  417 train loss :  0.04282281429784772  val loss :  0.07762408836921654\n",
      "epoch :  418 train loss :  0.041739475216072894  val loss :  0.07596143346025129\n",
      "epoch :  419 train loss :  0.04321093514015537  val loss :  0.07341164781707385\n",
      "epoch :  420 train loss :  0.04318721414938647  val loss :  0.07292211706240634\n",
      "epoch :  421 train loss :  0.04253287439805051  val loss :  0.07519993112661119\n",
      "epoch :  422 train loss :  0.04235036414682241  val loss :  0.07660302278106816\n",
      "epoch :  423 train loss :  0.04218781325010417  val loss :  0.07485680793770641\n",
      "epoch :  424 train loss :  0.04233135247885869  val loss :  0.07378335708586513\n",
      "epoch :  425 train loss :  0.04260502039840084  val loss :  0.07528911766453213\n",
      "epoch :  426 train loss :  0.04225298862767781  val loss :  0.07576513676592933\n",
      "epoch :  427 train loss :  0.04292645417543918  val loss :  0.07309385104477176\n",
      "epoch :  428 train loss :  0.04302618184482864  val loss :  0.0731123206319488\n",
      "epoch :  429 train loss :  0.04336089264425932  val loss :  0.07728725478299756\n",
      "epoch :  430 train loss :  0.04239481335496091  val loss :  0.07592890447987956\n",
      "epoch :  431 train loss :  0.0425400825617201  val loss :  0.07358113130511909\n",
      "epoch :  432 train loss :  0.042740358936927084  val loss :  0.07705735339146882\n",
      "epoch :  433 train loss :  0.04232959669687985  val loss :  0.07488923607564688\n",
      "epoch :  434 train loss :  0.0433867962845645  val loss :  0.0718347660436884\n",
      "epoch :  435 train loss :  0.04254229062514779  val loss :  0.07709690783811689\n",
      "epoch :  436 train loss :  0.04301434564184768  val loss :  0.07509016076795942\n",
      "epoch :  437 train loss :  0.04256501360869532  val loss :  0.07286229090304644\n",
      "epoch :  438 train loss :  0.04276758657661096  val loss :  0.07767902211229072\n",
      "epoch :  439 train loss :  0.04238566226711136  val loss :  0.071095861678097\n",
      "epoch :  440 train loss :  0.04306604435267561  val loss :  0.07422230921339619\n",
      "epoch :  441 train loss :  0.04386250809303129  val loss :  0.07885546650252556\n",
      "epoch :  442 train loss :  0.04294037525371419  val loss :  0.07159828068488604\n",
      "epoch :  443 train loss :  0.04352062501754436  val loss :  0.07692621698465295\n",
      "epoch :  444 train loss :  0.042896222762022346  val loss :  0.07597869042033965\n",
      "epoch :  445 train loss :  0.0433253020399216  val loss :  0.06975001346672617\n",
      "epoch :  446 train loss :  0.043575565940188486  val loss :  0.07943184331194818\n",
      "epoch :  447 train loss :  0.044610188698581375  val loss :  0.07998064629779524\n",
      "epoch :  448 train loss :  0.04299563716570432  val loss :  0.06932796357673712\n",
      "epoch :  449 train loss :  0.04405201351299336  val loss :  0.07283254621215027\n",
      "epoch :  450 train loss :  0.04338434408784537  val loss :  0.08106172521264951\n",
      "epoch :  451 train loss :  0.04342211190006496  val loss :  0.07614148462526278\n",
      "epoch :  452 train loss :  0.04318834231747071  val loss :  0.0695869757994545\n",
      "epoch :  453 train loss :  0.042628124083211906  val loss :  0.07240406042028283\n",
      "epoch :  454 train loss :  0.04294246694570437  val loss :  0.07991542532430018\n",
      "epoch :  455 train loss :  0.04386079929652014  val loss :  0.08083059540262869\n",
      "epoch :  456 train loss :  0.043187447808955975  val loss :  0.07364301082515619\n",
      "epoch :  457 train loss :  0.04350345307736809  val loss :  0.07036000103381403\n",
      "epoch :  458 train loss :  0.04415110816584208  val loss :  0.0729224303074018\n",
      "epoch :  459 train loss :  0.0426430037898543  val loss :  0.07658784974917958\n",
      "epoch :  460 train loss :  0.04251127285365971  val loss :  0.07940319884138523\n",
      "epoch :  461 train loss :  0.042976830066888745  val loss :  0.07928560959238767\n",
      "epoch :  462 train loss :  0.043112183301782736  val loss :  0.07451809966091832\n",
      "epoch :  463 train loss :  0.0420445760916821  val loss :  0.0714336382767611\n",
      "epoch :  464 train loss :  0.04290899247204134  val loss :  0.07191691175852395\n",
      "epoch :  465 train loss :  0.04257292510132203  val loss :  0.07557604788850975\n",
      "epoch :  466 train loss :  0.043260545289407225  val loss :  0.07914685527268177\n",
      "epoch :  467 train loss :  0.04277262249620173  val loss :  0.08057775932115198\n",
      "epoch :  468 train loss :  0.043379701020595914  val loss :  0.07710921537000104\n",
      "epoch :  469 train loss :  0.0434431918940619  val loss :  0.07457757106922235\n",
      "epoch :  470 train loss :  0.044206782483071555  val loss :  0.07295612213841529\n",
      "epoch :  471 train loss :  0.043854491044011415  val loss :  0.07505580270742147\n",
      "epoch :  472 train loss :  0.04391434895976676  val loss :  0.0805456597657095\n",
      "epoch :  473 train loss :  0.043363105603225566  val loss :  0.08101040725570315\n",
      "epoch :  474 train loss :  0.04419661873064116  val loss :  0.08149898783960795\n",
      "epoch :  475 train loss :  0.045941182777909706  val loss :  0.0763346226871507\n",
      "epoch :  476 train loss :  0.045601859835039886  val loss :  0.07667500566775302\n",
      "epoch :  477 train loss :  0.04408240865389402  val loss :  0.07903636524962068\n",
      "epoch :  478 train loss :  0.045871885981001156  val loss :  0.07879877686650391\n",
      "epoch :  479 train loss :  0.04373767801331288  val loss :  0.08214402552273374\n",
      "epoch :  480 train loss :  0.04591724883121346  val loss :  0.08056705177901083\n",
      "epoch :  481 train loss :  0.0442797445267907  val loss :  0.07899512485877638\n",
      "epoch :  482 train loss :  0.04409762053550538  val loss :  0.07047308661737525\n",
      "epoch :  483 train loss :  0.04478930879910891  val loss :  0.07428483905024033\n",
      "epoch :  484 train loss :  0.04352650751423149  val loss :  0.08497670793152036\n",
      "epoch :  485 train loss :  0.04517426490978733  val loss :  0.07850927359708078\n",
      "epoch :  486 train loss :  0.044494289280700434  val loss :  0.06949110601106279\n",
      "epoch :  487 train loss :  0.046377944764900585  val loss :  0.07023301837574818\n",
      "epoch :  488 train loss :  0.043751683642024766  val loss :  0.08177740433418174\n",
      "epoch :  489 train loss :  0.04568205389598901  val loss :  0.08207137258259446\n",
      "epoch :  490 train loss :  0.04442401403922061  val loss :  0.07539480107681973\n",
      "epoch :  491 train loss :  0.04527626512328368  val loss :  0.07039944580433392\n",
      "epoch :  492 train loss :  0.04438642754721704  val loss :  0.07708734913387953\n",
      "epoch :  493 train loss :  0.045072881641422265  val loss :  0.08346200324560396\n",
      "epoch :  494 train loss :  0.044254153215994387  val loss :  0.0850473530751902\n",
      "epoch :  495 train loss :  0.04602408890870853  val loss :  0.07780606807875193\n",
      "epoch :  496 train loss :  0.04315344618721158  val loss :  0.07389025121989504\n",
      "epoch :  497 train loss :  0.046503408758350066  val loss :  0.07804505800408205\n",
      "epoch :  498 train loss :  0.04349662963281439  val loss :  0.07956071609676134\n",
      "epoch :  499 train loss :  0.044123029940961546  val loss :  0.08355773117544249\n",
      "epoch :  500 train loss :  0.04625473589836303  val loss :  0.08019866139886181\n",
      "epoch :  501 train loss :  0.043274830275883226  val loss :  0.07399823597060645\n",
      "epoch :  502 train loss :  0.04487245497708233  val loss :  0.07103752725271249\n",
      "epoch :  503 train loss :  0.0460254594937201  val loss :  0.06989251075936066\n",
      "epoch :  504 train loss :  0.04281999967524206  val loss :  0.0770181757095385\n",
      "epoch :  505 train loss :  0.0448958375515582  val loss :  0.08431343890531773\n",
      "epoch :  506 train loss :  0.04489687672226217  val loss :  0.07961625937345936\n",
      "epoch :  507 train loss :  0.04244037027134321  val loss :  0.07095990315832354\n",
      "epoch :  508 train loss :  0.0442239600989519  val loss :  0.06905530572299486\n",
      "epoch :  509 train loss :  0.04421916749347879  val loss :  0.07419338470237274\n",
      "epoch :  510 train loss :  0.04470380702841032  val loss :  0.08138987411615799\n",
      "epoch :  511 train loss :  0.04451617083624396  val loss :  0.08459143492633518\n",
      "epoch :  512 train loss :  0.04431550371514253  val loss :  0.08239153177808223\n",
      "epoch :  513 train loss :  0.04264551700526819  val loss :  0.07555487940703418\n",
      "epoch :  514 train loss :  0.04386591552439784  val loss :  0.06853260610530403\n",
      "epoch :  515 train loss :  0.044647528843573876  val loss :  0.06782623141756774\n",
      "epoch :  516 train loss :  0.042882358196986284  val loss :  0.07305811533498656\n",
      "epoch :  517 train loss :  0.04334859800627407  val loss :  0.0793074145473685\n",
      "epoch :  518 train loss :  0.04390645356780572  val loss :  0.08242215099744499\n",
      "epoch :  519 train loss :  0.04348946204750326  val loss :  0.08148120987283222\n",
      "epoch :  520 train loss :  0.04333633737375287  val loss :  0.07543970939085477\n",
      "epoch :  521 train loss :  0.04264251935583447  val loss :  0.07030900863908683\n",
      "epoch :  522 train loss :  0.04388339293837859  val loss :  0.07195968664306594\n",
      "epoch :  523 train loss :  0.04340196633955259  val loss :  0.07472558121662587\n",
      "epoch :  524 train loss :  0.043189438966868436  val loss :  0.07864801700112133\n",
      "epoch :  525 train loss :  0.043058502990069816  val loss :  0.0823830476498674\n",
      "epoch :  526 train loss :  0.04378232688530889  val loss :  0.08036668667262005\n",
      "epoch :  527 train loss :  0.04278811787045439  val loss :  0.07536396432620714\n",
      "epoch :  528 train loss :  0.041955300221580484  val loss :  0.07101021304783944\n",
      "epoch :  529 train loss :  0.04268011940321373  val loss :  0.06934890464826551\n",
      "epoch :  530 train loss :  0.04242371118271538  val loss :  0.07173226726020106\n",
      "epoch :  531 train loss :  0.042362997357133794  val loss :  0.0771488018361575\n",
      "epoch :  532 train loss :  0.04392662257229158  val loss :  0.07917041940635329\n",
      "epoch :  533 train loss :  0.04349351855706794  val loss :  0.07874663713697048\n",
      "epoch :  534 train loss :  0.0435824908257623  val loss :  0.07781097375543437\n",
      "epoch :  535 train loss :  0.04324090654391269  val loss :  0.07711114006957756\n",
      "epoch :  536 train loss :  0.04158363433759562  val loss :  0.07242273716232969\n",
      "epoch :  537 train loss :  0.043492700260971226  val loss :  0.07020746112171801\n",
      "epoch :  538 train loss :  0.04431678784806379  val loss :  0.073757742892658\n",
      "epoch :  539 train loss :  0.04283182485838523  val loss :  0.08004501427821746\n",
      "epoch :  540 train loss :  0.04300301922826555  val loss :  0.08112506042164001\n",
      "epoch :  541 train loss :  0.04247797811062548  val loss :  0.07496824839839937\n",
      "epoch :  542 train loss :  0.042346842128646936  val loss :  0.07005615945269197\n",
      "epoch :  543 train loss :  0.0429972290739183  val loss :  0.07281732767469383\n",
      "epoch :  544 train loss :  0.04247024431044519  val loss :  0.08043200698782607\n",
      "epoch :  545 train loss :  0.042384569460028755  val loss :  0.08128927558753293\n",
      "epoch :  546 train loss :  0.04220003375286207  val loss :  0.07411351684889511\n",
      "epoch :  547 train loss :  0.04332113437627622  val loss :  0.0722307116087148\n",
      "epoch :  548 train loss :  0.042601827863623334  val loss :  0.07798368417909185\n",
      "epoch :  549 train loss :  0.042115160805314626  val loss :  0.07811573695170689\n",
      "epoch :  550 train loss :  0.0429907058077957  val loss :  0.07637283142555522\n",
      "epoch :  551 train loss :  0.04231881152311857  val loss :  0.07705597126842614\n",
      "epoch :  552 train loss :  0.04297398858708549  val loss :  0.07445299253271118\n",
      "epoch :  553 train loss :  0.04276349714406186  val loss :  0.07222086835481424\n",
      "epoch :  554 train loss :  0.04125239195897005  val loss :  0.08013609931763897\n",
      "epoch :  555 train loss :  0.043546183190810744  val loss :  0.08052066993069176\n",
      "epoch :  556 train loss :  0.04244074875888712  val loss :  0.07134466423606385\n",
      "epoch :  557 train loss :  0.042649112661315505  val loss :  0.07150451842485697\n",
      "epoch :  558 train loss :  0.04249976471027467  val loss :  0.0785866716218476\n",
      "epoch :  559 train loss :  0.042936488722709464  val loss :  0.07811894639181077\n",
      "epoch :  560 train loss :  0.0427798212783811  val loss :  0.07509142633021514\n",
      "epoch :  561 train loss :  0.0438881082001937  val loss :  0.07705446066407459\n",
      "epoch :  562 train loss :  0.0422166634194982  val loss :  0.07441379677787847\n",
      "epoch :  563 train loss :  0.04278632109077813  val loss :  0.07037456822898562\n",
      "epoch :  564 train loss :  0.04242486038601211  val loss :  0.07705010435172809\n",
      "epoch :  565 train loss :  0.04257007222135029  val loss :  0.08081783847131255\n",
      "epoch :  566 train loss :  0.042023006300027456  val loss :  0.07266781760188336\n",
      "epoch :  567 train loss :  0.042781425688747335  val loss :  0.07405168209904989\n",
      "epoch :  568 train loss :  0.04250418757580962  val loss :  0.0760178763758262\n",
      "epoch :  569 train loss :  0.041511850226768024  val loss :  0.07275552559512297\n",
      "epoch :  570 train loss :  0.04268185900651036  val loss :  0.079927963436576\n",
      "epoch :  571 train loss :  0.04213031113186744  val loss :  0.07920559825771027\n",
      "epoch :  572 train loss :  0.041738459625871395  val loss :  0.06955700719845481\n",
      "epoch :  573 train loss :  0.042790431334242145  val loss :  0.07568225487928358\n",
      "epoch :  574 train loss :  0.04221300811004576  val loss :  0.07912564670320112\n",
      "epoch :  575 train loss :  0.04251450072993471  val loss :  0.07483117610507467\n",
      "epoch :  576 train loss :  0.041559656048242334  val loss :  0.07912124701058676\n",
      "epoch :  577 train loss :  0.04337791991249429  val loss :  0.07217725444156307\n",
      "epoch :  578 train loss :  0.042770355296228565  val loss :  0.07163341836302166\n",
      "epoch :  579 train loss :  0.042503910714575134  val loss :  0.08426265999684018\n",
      "epoch :  580 train loss :  0.043083073444547454  val loss :  0.07686901084590579\n",
      "epoch :  581 train loss :  0.041904798762492486  val loss :  0.06939176960217497\n",
      "epoch :  582 train loss :  0.043228828713921974  val loss :  0.07679406216651626\n",
      "epoch :  583 train loss :  0.04350031445202715  val loss :  0.0769147390587887\n",
      "epoch :  584 train loss :  0.04378973681928287  val loss :  0.07642507916098831\n",
      "epoch :  585 train loss :  0.04213408762052733  val loss :  0.08150705018041582\n",
      "epoch :  586 train loss :  0.044030012406837876  val loss :  0.07077083725938665\n",
      "epoch :  587 train loss :  0.04435460879930651  val loss :  0.06881641158456614\n",
      "epoch :  588 train loss :  0.04275136963703245  val loss :  0.08313166652993405\n",
      "epoch :  589 train loss :  0.04404718698982481  val loss :  0.08639819144114556\n",
      "epoch :  590 train loss :  0.04190687590439594  val loss :  0.07353705087007256\n",
      "epoch :  591 train loss :  0.04261374007422886  val loss :  0.06952211651709023\n",
      "epoch :  592 train loss :  0.04368057880180044  val loss :  0.07323810099773939\n",
      "epoch :  593 train loss :  0.042694910434294124  val loss :  0.07392504228590781\n",
      "epoch :  594 train loss :  0.043302741954932035  val loss :  0.08013985766617882\n",
      "epoch :  595 train loss :  0.043854583405855435  val loss :  0.08830005009941028\n",
      "epoch :  596 train loss :  0.04478794484550416  val loss :  0.08092779992033017\n",
      "epoch :  597 train loss :  0.04236399322865209  val loss :  0.06925764487773044\n",
      "epoch :  598 train loss :  0.04450810688952501  val loss :  0.06864242184138396\n",
      "epoch :  599 train loss :  0.04305655856407126  val loss :  0.07691606916521795\n",
      "epoch :  600 train loss :  0.04383003385006133  val loss :  0.08011196406876857\n",
      "epoch :  601 train loss :  0.04242859294160186  val loss :  0.07925799483272575\n",
      "epoch :  602 train loss :  0.04378182546318513  val loss :  0.07811018905290033\n",
      "epoch :  603 train loss :  0.04338402374213591  val loss :  0.0816215793772181\n",
      "epoch :  604 train loss :  0.04468565081192561  val loss :  0.0779533929841035\n",
      "epoch :  605 train loss :  0.041679101571908794  val loss :  0.07428574403471286\n",
      "epoch :  606 train loss :  0.043315848238103055  val loss :  0.0774455277876246\n",
      "epoch :  607 train loss :  0.04380515763896922  val loss :  0.07944000084967454\n",
      "epoch :  608 train loss :  0.042929925395088046  val loss :  0.07511347281172558\n",
      "epoch :  609 train loss :  0.042654887934005695  val loss :  0.06939454390399394\n",
      "epoch :  610 train loss :  0.04334606604738385  val loss :  0.07168512134160097\n",
      "epoch :  611 train loss :  0.0420769043656856  val loss :  0.07989336675122878\n",
      "epoch :  612 train loss :  0.04304964534197178  val loss :  0.08635188813383535\n",
      "epoch :  613 train loss :  0.043711198259788656  val loss :  0.08372983812843796\n",
      "epoch :  614 train loss :  0.042790533877246045  val loss :  0.07643472271686706\n",
      "epoch :  615 train loss :  0.0425806547763772  val loss :  0.07167972664676271\n",
      "epoch :  616 train loss :  0.04215482039211308  val loss :  0.07280258881245132\n",
      "epoch :  617 train loss :  0.042614258650241724  val loss :  0.07883957396019342\n",
      "epoch :  618 train loss :  0.04265842702805372  val loss :  0.07734582928487095\n",
      "epoch :  619 train loss :  0.04182892046010619  val loss :  0.07115151373727713\n",
      "epoch :  620 train loss :  0.04217046056508394  val loss :  0.07210068278922929\n",
      "epoch :  621 train loss :  0.04248775812889893  val loss :  0.08293530057469096\n",
      "epoch :  622 train loss :  0.04268791510952705  val loss :  0.08591329794083745\n",
      "epoch :  623 train loss :  0.04283146257877974  val loss :  0.07569413287086772\n",
      "epoch :  624 train loss :  0.043584915443630745  val loss :  0.07101131687090631\n",
      "epoch :  625 train loss :  0.04158701090366428  val loss :  0.07582126198985412\n",
      "epoch :  626 train loss :  0.04239558562129268  val loss :  0.08102760503437695\n",
      "epoch :  627 train loss :  0.043336118771377656  val loss :  0.07473027615146342\n",
      "epoch :  628 train loss :  0.04276464512834998  val loss :  0.06816866366414003\n",
      "epoch :  629 train loss :  0.04353419228100964  val loss :  0.07004443406833953\n",
      "epoch :  630 train loss :  0.04155907407402992  val loss :  0.07907240252479643\n",
      "epoch :  631 train loss :  0.04267828820848652  val loss :  0.08512576557628149\n",
      "epoch :  632 train loss :  0.04282371943611749  val loss :  0.07916323591304925\n",
      "epoch :  633 train loss :  0.0414410712909324  val loss :  0.07172582599583387\n",
      "epoch :  634 train loss :  0.042922803420202896  val loss :  0.07316418650207378\n",
      "epoch :  635 train loss :  0.04157244238553871  val loss :  0.07822329522626899\n",
      "epoch :  636 train loss :  0.04290985486699336  val loss :  0.0821191886865527\n",
      "epoch :  637 train loss :  0.042178401226148556  val loss :  0.07542356274132202\n",
      "epoch :  638 train loss :  0.042047966486184384  val loss :  0.06879954316018214\n",
      "epoch :  639 train loss :  0.041921821817833715  val loss :  0.07115698189965661\n",
      "epoch :  640 train loss :  0.042238613093242595  val loss :  0.08186238006565323\n",
      "epoch :  641 train loss :  0.04275888248458895  val loss :  0.08261509300431462\n",
      "epoch :  642 train loss :  0.0425433855289252  val loss :  0.08363606460308333\n",
      "epoch :  643 train loss :  0.04464380173939061  val loss :  0.06753153056530155\n",
      "epoch :  644 train loss :  0.04477907159409598  val loss :  0.07934484073787347\n",
      "epoch :  645 train loss :  0.04273151137246824  val loss :  0.07604297755993811\n",
      "epoch :  646 train loss :  0.04451949767651358  val loss :  0.09145779265661373\n",
      "epoch :  647 train loss :  0.04426167330933803  val loss :  0.07396051198725893\n",
      "epoch :  648 train loss :  0.04482737693604062  val loss :  0.07128468584695757\n",
      "epoch :  649 train loss :  0.04491530385121937  val loss :  0.07912233685308118\n",
      "epoch :  650 train loss :  0.04437002779535598  val loss :  0.06962238922177774\n",
      "epoch :  651 train loss :  0.04414652906241217  val loss :  0.07318252979706867\n",
      "epoch :  652 train loss :  0.04312981267250021  val loss :  0.08264383165946543\n",
      "epoch :  653 train loss :  0.04352260258107285  val loss :  0.08966674762495591\n",
      "epoch :  654 train loss :  0.04446571622609468  val loss :  0.08078933371525455\n",
      "epoch :  655 train loss :  0.0414779204485148  val loss :  0.07111845928531946\n",
      "epoch :  656 train loss :  0.043438359080808946  val loss :  0.06779252476613098\n",
      "epoch :  657 train loss :  0.04494915565193011  val loss :  0.0690932979260192\n",
      "epoch :  658 train loss :  0.04320288557773797  val loss :  0.07374189745316453\n",
      "epoch :  659 train loss :  0.04508641696686208  val loss :  0.0789980418109312\n",
      "epoch :  660 train loss :  0.043785165956351145  val loss :  0.0808469212674981\n",
      "epoch :  661 train loss :  0.043356072157621384  val loss :  0.07856256994035551\n",
      "epoch :  662 train loss :  0.0447603337642254  val loss :  0.07723308612660523\n",
      "epoch :  663 train loss :  0.04391530920697757  val loss :  0.08105172406297666\n",
      "epoch :  664 train loss :  0.04425296785202638  val loss :  0.08107524287297925\n",
      "epoch :  665 train loss :  0.042024576295342744  val loss :  0.0719746439271067\n",
      "epoch :  666 train loss :  0.04256790448272728  val loss :  0.0708553267226181\n",
      "epoch :  667 train loss :  0.04203753158723184  val loss :  0.07777246010088071\n",
      "epoch :  668 train loss :  0.043194258224745694  val loss :  0.07765729937422618\n",
      "epoch :  669 train loss :  0.043146123387972725  val loss :  0.07394733668205854\n",
      "epoch :  670 train loss :  0.042486001908077  val loss :  0.07699887567776945\n",
      "epoch :  671 train loss :  0.043199107509242926  val loss :  0.07321281016456761\n",
      "epoch :  672 train loss :  0.0426832978950121  val loss :  0.06867766168089826\n",
      "epoch :  673 train loss :  0.043195146150613956  val loss :  0.08313573459462992\n",
      "epoch :  674 train loss :  0.042771811280107  val loss :  0.07844438144042719\n",
      "epoch :  675 train loss :  0.044727008897050516  val loss :  0.07969731872262766\n",
      "epoch :  676 train loss :  0.04322257418556051  val loss :  0.08792615600972262\n",
      "epoch :  677 train loss :  0.04288043375297679  val loss :  0.07004756989455459\n",
      "epoch :  678 train loss :  0.04409504074271749  val loss :  0.07249780022812947\n",
      "epoch :  679 train loss :  0.04592899161912696  val loss :  0.07639832208007989\n",
      "epoch :  680 train loss :  0.0430175822462243  val loss :  0.0710371673218952\n",
      "epoch :  681 train loss :  0.04516939619916896  val loss :  0.07642727880205585\n",
      "epoch :  682 train loss :  0.04452728099769947  val loss :  0.08857066350519925\n",
      "epoch :  683 train loss :  0.045478722265877645  val loss :  0.08168383313937684\n",
      "epoch :  684 train loss :  0.043061712795761244  val loss :  0.07321791570474587\n",
      "epoch :  685 train loss :  0.04565987341027922  val loss :  0.07226531486576458\n",
      "epoch :  686 train loss :  0.041131683630156894  val loss :  0.07572020437783968\n",
      "epoch :  687 train loss :  0.042776718823463504  val loss :  0.07676523220612977\n",
      "epoch :  688 train loss :  0.04319094612960416  val loss :  0.07525905389229592\n",
      "epoch :  689 train loss :  0.04400625682548079  val loss :  0.07380168665950256\n",
      "epoch :  690 train loss :  0.04448680793232631  val loss :  0.07362158964030759\n",
      "epoch :  691 train loss :  0.042927262836491877  val loss :  0.07486992658045037\n",
      "epoch :  692 train loss :  0.0436777584449783  val loss :  0.08185542551518216\n",
      "epoch :  693 train loss :  0.04408854744470244  val loss :  0.07987444217032684\n",
      "epoch :  694 train loss :  0.04417568499817274  val loss :  0.0705594095032049\n",
      "epoch :  695 train loss :  0.04586258047889352  val loss :  0.07356869675421344\n",
      "epoch :  696 train loss :  0.04422911510105532  val loss :  0.08521115233834099\n",
      "epoch :  697 train loss :  0.04315320473573907  val loss :  0.0766025027151225\n",
      "epoch :  698 train loss :  0.04452496690002723  val loss :  0.07388220600107233\n",
      "epoch :  699 train loss :  0.04469018125253198  val loss :  0.07764183452219177\n",
      "epoch :  700 train loss :  0.04549524905793954  val loss :  0.07100194452555521\n",
      "epoch :  701 train loss :  0.045322649249153614  val loss :  0.07134736300994887\n",
      "epoch :  702 train loss :  0.04357329529773502  val loss :  0.07925891897774827\n",
      "epoch :  703 train loss :  0.04471961350341118  val loss :  0.08182040772154134\n",
      "epoch :  704 train loss :  0.04269791177976194  val loss :  0.07392426211566153\n",
      "epoch :  705 train loss :  0.04516884604595719  val loss :  0.07376285136501116\n",
      "epoch :  706 train loss :  0.043508089952053824  val loss :  0.08609124322754381\n",
      "epoch :  707 train loss :  0.04506442088302205  val loss :  0.08020354298175092\n",
      "epoch :  708 train loss :  0.0430830941189334  val loss :  0.07170599378885455\n",
      "epoch :  709 train loss :  0.044207184356076554  val loss :  0.07735491480469694\n",
      "epoch :  710 train loss :  0.04247840562181947  val loss :  0.08495361348338908\n",
      "epoch :  711 train loss :  0.04357684540186877  val loss :  0.07635132846247294\n",
      "epoch :  712 train loss :  0.0425096184151803  val loss :  0.06652615754641789\n",
      "epoch :  713 train loss :  0.045234683910823616  val loss :  0.06867600728741542\n",
      "epoch :  714 train loss :  0.0429312989252244  val loss :  0.08125392105724284\n",
      "epoch :  715 train loss :  0.04452464017646475  val loss :  0.08178001529709171\n",
      "epoch :  716 train loss :  0.0412461728891309  val loss :  0.06951531972508476\n",
      "epoch :  717 train loss :  0.04416420631578768  val loss :  0.07202522989822377\n",
      "epoch :  718 train loss :  0.04312925311829407  val loss :  0.09111427169323133\n",
      "epoch :  719 train loss :  0.043093772819607044  val loss :  0.07956189617373702\n",
      "epoch :  720 train loss :  0.04457939262087433  val loss :  0.0721260158925451\n",
      "epoch :  721 train loss :  0.04215461068362466  val loss :  0.07849519355147662\n",
      "epoch :  722 train loss :  0.043631236083532504  val loss :  0.08125234647979038\n",
      "epoch :  723 train loss :  0.04356662472661253  val loss :  0.0717933312579148\n",
      "epoch :  724 train loss :  0.043565387900977234  val loss :  0.07065472830609086\n",
      "epoch :  725 train loss :  0.044116024522803214  val loss :  0.07303673098463735\n",
      "epoch :  726 train loss :  0.04240386229964139  val loss :  0.07189672097232651\n",
      "epoch :  727 train loss :  0.042082945450516274  val loss :  0.07533648108434172\n",
      "epoch :  728 train loss :  0.042212619636383356  val loss :  0.08305397846201194\n",
      "epoch :  729 train loss :  0.04256431370271438  val loss :  0.08151081665955014\n",
      "epoch :  730 train loss :  0.04234434462381595  val loss :  0.07375597002072315\n",
      "epoch :  731 train loss :  0.043394987993137374  val loss :  0.08307042461994642\n",
      "epoch :  732 train loss :  0.044224312704505095  val loss :  0.07526797652952713\n",
      "epoch :  733 train loss :  0.04394409915734648  val loss :  0.0738657051417141\n",
      "epoch :  734 train loss :  0.04307136114160116  val loss :  0.08060608190979758\n",
      "epoch :  735 train loss :  0.043420323770473765  val loss :  0.06742310272771773\n",
      "epoch :  736 train loss :  0.04367485183110724  val loss :  0.08007303159907264\n",
      "epoch :  737 train loss :  0.044413757924946194  val loss :  0.072990679974111\n",
      "epoch :  738 train loss :  0.04574424316082638  val loss :  0.06826863052567818\n",
      "epoch :  739 train loss :  0.04503336352788653  val loss :  0.08717322839020379\n",
      "epoch :  740 train loss :  0.04448872967501273  val loss :  0.0700247992829438\n",
      "epoch :  741 train loss :  0.04457763574666378  val loss :  0.06941602278632195\n",
      "epoch :  742 train loss :  0.04208342652982442  val loss :  0.08883713390356873\n",
      "epoch :  743 train loss :  0.04366364162317745  val loss :  0.07908429652348732\n",
      "epoch :  744 train loss :  0.042381771952303915  val loss :  0.06985565445254474\n",
      "epoch :  745 train loss :  0.04280881798236158  val loss :  0.08097575771988895\n",
      "epoch :  746 train loss :  0.044234931712765345  val loss :  0.09100754345847308\n",
      "epoch :  747 train loss :  0.04349639985456829  val loss :  0.07388138067061405\n",
      "epoch :  748 train loss :  0.044028847093123415  val loss :  0.06926224483430644\n",
      "epoch :  749 train loss :  0.04333967043584242  val loss :  0.08061886772497252\n",
      "epoch :  750 train loss :  0.04311995133172467  val loss :  0.08617044461598287\n",
      "epoch :  751 train loss :  0.04453256495140922  val loss :  0.07560929589579804\n",
      "epoch :  752 train loss :  0.04344883254919377  val loss :  0.07037457144740822\n",
      "epoch :  753 train loss :  0.04637738307499137  val loss :  0.07196916069081606\n",
      "epoch :  754 train loss :  0.04348443520708858  val loss :  0.07807481648270179\n",
      "epoch :  755 train loss :  0.045880929318242045  val loss :  0.08416683702282424\n",
      "epoch :  756 train loss :  0.0445456185344002  val loss :  0.08102487377294496\n",
      "epoch :  757 train loss :  0.04407566699018965  val loss :  0.06960595182793233\n",
      "epoch :  758 train loss :  0.043464271224918166  val loss :  0.06673955329374576\n",
      "epoch :  759 train loss :  0.04327648190576681  val loss :  0.0766880263336404\n",
      "epoch :  760 train loss :  0.04390549944489414  val loss :  0.08844493249590316\n",
      "epoch :  761 train loss :  0.04322236108335213  val loss :  0.0831629456826866\n",
      "epoch :  762 train loss :  0.04291443124959606  val loss :  0.07291087401850152\n",
      "epoch :  763 train loss :  0.04287928258951422  val loss :  0.06970227624740369\n",
      "epoch :  764 train loss :  0.042778549491579  val loss :  0.07635170068259876\n",
      "epoch :  765 train loss :  0.04339233999437999  val loss :  0.07867604613737526\n",
      "epoch :  766 train loss :  0.041842095202800494  val loss :  0.07505874323202205\n",
      "epoch :  767 train loss :  0.042021712522310115  val loss :  0.0763506809217679\n",
      "epoch :  768 train loss :  0.042481702113650856  val loss :  0.07898460160367454\n",
      "epoch :  769 train loss :  0.043042756587813036  val loss :  0.07910941277888095\n",
      "epoch :  770 train loss :  0.04279281743143865  val loss :  0.07587333483925339\n",
      "epoch :  771 train loss :  0.042012282213225415  val loss :  0.07393722018468457\n",
      "epoch :  772 train loss :  0.04309490230422058  val loss :  0.07620036577541349\n",
      "epoch :  773 train loss :  0.042453008640499015  val loss :  0.07772952635538544\n",
      "epoch :  774 train loss :  0.04169803921191792  val loss :  0.07767105626350379\n",
      "epoch :  775 train loss :  0.04374096934629985  val loss :  0.08337213534321221\n",
      "epoch :  776 train loss :  0.04266006096768442  val loss :  0.08178812031239953\n",
      "epoch :  777 train loss :  0.042361746400750744  val loss :  0.07272667870257456\n",
      "epoch :  778 train loss :  0.04315448364155143  val loss :  0.07922701076212532\n",
      "epoch :  779 train loss :  0.04360565897174843  val loss :  0.08293490770006653\n",
      "epoch :  780 train loss :  0.044432171391254945  val loss :  0.07891596965768402\n",
      "epoch :  781 train loss :  0.04271508949316297  val loss :  0.08184210160668792\n",
      "epoch :  782 train loss :  0.043956707607155074  val loss :  0.07009933045823219\n",
      "epoch :  783 train loss :  0.0431320123784049  val loss :  0.08691051426242188\n",
      "epoch :  784 train loss :  0.043666892241277  val loss :  0.0743515528969024\n",
      "epoch :  785 train loss :  0.043939624547334244  val loss :  0.08286381383843738\n",
      "epoch :  786 train loss :  0.04359890813134728  val loss :  0.07086363429489481\n",
      "epoch :  787 train loss :  0.04527976021599707  val loss :  0.07318273659289519\n",
      "epoch :  788 train loss :  0.04462180074050789  val loss :  0.08990540703998344\n",
      "epoch :  789 train loss :  0.04461608741295899  val loss :  0.06900837447864262\n",
      "epoch :  790 train loss :  0.04523308460078938  val loss :  0.08598986328585248\n",
      "epoch :  791 train loss :  0.046405135395483195  val loss :  0.08041388753990039\n",
      "epoch :  792 train loss :  0.04510796163718738  val loss :  0.06551970487764787\n",
      "epoch :  793 train loss :  0.04524528854453439  val loss :  0.08833186371461718\n",
      "epoch :  794 train loss :  0.04857782453922701  val loss :  0.09778352126094171\n",
      "epoch :  795 train loss :  0.04344009137005394  val loss :  0.06657099101397365\n",
      "epoch :  796 train loss :  0.050162311038733774  val loss :  0.06613316958756324\n",
      "epoch :  797 train loss :  0.04369618664666308  val loss :  0.0899956693084989\n",
      "epoch :  798 train loss :  0.05046478194716089  val loss :  0.09988813282602632\n",
      "epoch :  799 train loss :  0.044151660025665895  val loss :  0.07632850587199728\n",
      "epoch :  800 train loss :  0.04633309344970743  val loss :  0.06670814193497034\n",
      "epoch :  801 train loss :  0.04676122313940712  val loss :  0.06762003592425256\n",
      "epoch :  802 train loss :  0.04289437745605152  val loss :  0.07858732715566034\n",
      "epoch :  803 train loss :  0.04552742641867767  val loss :  0.0888270088020507\n",
      "epoch :  804 train loss :  0.04652277152029632  val loss :  0.09026661431029298\n",
      "epoch :  805 train loss :  0.04569706604547838  val loss :  0.08314145092148159\n",
      "epoch :  806 train loss :  0.04293999615178994  val loss :  0.07098248390590951\n",
      "epoch :  807 train loss :  0.04428084472594149  val loss :  0.06533048946736322\n",
      "epoch :  808 train loss :  0.04626003726926774  val loss :  0.06811547066496529\n",
      "epoch :  809 train loss :  0.042626774406635946  val loss :  0.07977688802166445\n",
      "epoch :  810 train loss :  0.04434942157636762  val loss :  0.08929930229459217\n",
      "epoch :  811 train loss :  0.04509676001107817  val loss :  0.08979094237076767\n",
      "epoch :  812 train loss :  0.04332014670624783  val loss :  0.08078464562702108\n",
      "epoch :  813 train loss :  0.04466452206733651  val loss :  0.07321409562183083\n",
      "epoch :  814 train loss :  0.04478015754547419  val loss :  0.07835463030317955\n",
      "epoch :  815 train loss :  0.04530641123535433  val loss :  0.08254801643791028\n",
      "epoch :  816 train loss :  0.04294794186442622  val loss :  0.08279738627661032\n",
      "epoch :  817 train loss :  0.04528344588363982  val loss :  0.08449988689194064\n",
      "epoch :  818 train loss :  0.04541412180201858  val loss :  0.0758053256855185\n",
      "epoch :  819 train loss :  0.04207754829479138  val loss :  0.07220301774012312\n",
      "epoch :  820 train loss :  0.04654526548625911  val loss :  0.07323236457915921\n",
      "epoch :  821 train loss :  0.041963995575748814  val loss :  0.07739817851494207\n",
      "epoch :  822 train loss :  0.04576024724199822  val loss :  0.08532890559984993\n",
      "epoch :  823 train loss :  0.0458896120824895  val loss :  0.07752921085315986\n",
      "epoch :  824 train loss :  0.04254454970164761  val loss :  0.07044674725430825\n",
      "epoch :  825 train loss :  0.047735472458429365  val loss :  0.06976974082103295\n",
      "epoch :  826 train loss :  0.04369248817455394  val loss :  0.07218440430510287\n",
      "epoch :  827 train loss :  0.042752983214343405  val loss :  0.08195881463491039\n",
      "epoch :  828 train loss :  0.04574907659821173  val loss :  0.08770906355803704\n",
      "epoch :  829 train loss :  0.0468052140783265  val loss :  0.08235328541209894\n",
      "epoch :  830 train loss :  0.043510842892824045  val loss :  0.0753565525207732\n",
      "epoch :  831 train loss :  0.04183407800749959  val loss :  0.07463374315463651\n",
      "epoch :  832 train loss :  0.04394084742751109  val loss :  0.07471814531012264\n",
      "epoch :  833 train loss :  0.0441013453382004  val loss :  0.07029340600325164\n",
      "epoch :  834 train loss :  0.04383548378398281  val loss :  0.0751013423239531\n",
      "epoch :  835 train loss :  0.044259593407599086  val loss :  0.08661545126117987\n",
      "epoch :  836 train loss :  0.04432328708508876  val loss :  0.08019658195812689\n",
      "epoch :  837 train loss :  0.042497950562163794  val loss :  0.07287311477550319\n",
      "epoch :  838 train loss :  0.042986632505168466  val loss :  0.07533844168779745\n",
      "epoch :  839 train loss :  0.0441771364960995  val loss :  0.07241983630718654\n",
      "epoch :  840 train loss :  0.043714168652189966  val loss :  0.06785991869314739\n",
      "epoch :  841 train loss :  0.04380546839839501  val loss :  0.07605102582169596\n",
      "epoch :  842 train loss :  0.04291009392920901  val loss :  0.08449355517849752\n",
      "epoch :  843 train loss :  0.04331532203597236  val loss :  0.07907791640396267\n",
      "epoch :  844 train loss :  0.04349605277843813  val loss :  0.07425923007737072\n",
      "epoch :  845 train loss :  0.04332757113179611  val loss :  0.07350724409242151\n",
      "epoch :  846 train loss :  0.0442682341794381  val loss :  0.06809265823561657\n",
      "epoch :  847 train loss :  0.04233387608997797  val loss :  0.07240706734707358\n",
      "epoch :  848 train loss :  0.04254372960141816  val loss :  0.0812879509853296\n",
      "epoch :  849 train loss :  0.04393825608827369  val loss :  0.08048792797122624\n",
      "epoch :  850 train loss :  0.042841247872922435  val loss :  0.06958541074291674\n",
      "epoch :  851 train loss :  0.04375326277502856  val loss :  0.06641439219242559\n",
      "epoch :  852 train loss :  0.04302596427617273  val loss :  0.07279197550851317\n",
      "epoch :  853 train loss :  0.0427285993657031  val loss :  0.08225470443404888\n",
      "epoch :  854 train loss :  0.04248997309679136  val loss :  0.07911314682399342\n",
      "epoch :  855 train loss :  0.04249321932645993  val loss :  0.07052702616524449\n",
      "epoch :  856 train loss :  0.04353532376479728  val loss :  0.07293700962918143\n",
      "epoch :  857 train loss :  0.04220368711190074  val loss :  0.08460432617663749\n",
      "epoch :  858 train loss :  0.04251677018263577  val loss :  0.07941306200259397\n",
      "epoch :  859 train loss :  0.04193546770247802  val loss :  0.06811647672271863\n",
      "epoch :  860 train loss :  0.04261145665656522  val loss :  0.07071864110415359\n",
      "epoch :  861 train loss :  0.04210844163963308  val loss :  0.08042190605855205\n",
      "epoch :  862 train loss :  0.041818674722033024  val loss :  0.07547373093415678\n",
      "epoch :  863 train loss :  0.04405646559572657  val loss :  0.07469972338118835\n",
      "epoch :  864 train loss :  0.04167091733342066  val loss :  0.0824460516468216\n",
      "epoch :  865 train loss :  0.04238525200732716  val loss :  0.07484756999690602\n",
      "epoch :  866 train loss :  0.04302570905477901  val loss :  0.07370663438272422\n",
      "epoch :  867 train loss :  0.04301571695826441  val loss :  0.0876866650397631\n",
      "epoch :  868 train loss :  0.04341644183269346  val loss :  0.07826567087080422\n",
      "epoch :  869 train loss :  0.043550333047178404  val loss :  0.07599256451052641\n",
      "epoch :  870 train loss :  0.0436704396343356  val loss :  0.08638574200319842\n",
      "epoch :  871 train loss :  0.043111045430389995  val loss :  0.07082639234820565\n",
      "epoch :  872 train loss :  0.04422415462346913  val loss :  0.07583135725425483\n",
      "epoch :  873 train loss :  0.04228734623894329  val loss :  0.08970647172813331\n",
      "epoch :  874 train loss :  0.042664740332134105  val loss :  0.07396459877364209\n",
      "epoch :  875 train loss :  0.04440622098977965  val loss :  0.0674205812461914\n",
      "epoch :  876 train loss :  0.04304563849969372  val loss :  0.07582911558848077\n",
      "epoch :  877 train loss :  0.04319933900385315  val loss :  0.08332271780933964\n",
      "epoch :  878 train loss :  0.04161089260852774  val loss :  0.08011849894603866\n",
      "epoch :  879 train loss :  0.042031393118241694  val loss :  0.07602474363807457\n",
      "epoch :  880 train loss :  0.043305729042875205  val loss :  0.07551156918317688\n",
      "epoch :  881 train loss :  0.044331018066921156  val loss :  0.07748340713816906\n",
      "epoch :  882 train loss :  0.04090519461059133  val loss :  0.07546625065209234\n",
      "epoch :  883 train loss :  0.042354762602929044  val loss :  0.07422964892403158\n",
      "epoch :  884 train loss :  0.041380384390812895  val loss :  0.07678734172408765\n",
      "epoch :  885 train loss :  0.042705623832828714  val loss :  0.07576129388405324\n",
      "epoch :  886 train loss :  0.042499893310806516  val loss :  0.0710469816291548\n",
      "epoch :  887 train loss :  0.04274990752885479  val loss :  0.08117512549420053\n",
      "epoch :  888 train loss :  0.04390095816973928  val loss :  0.08080829168458949\n",
      "epoch :  889 train loss :  0.042890103436343335  val loss :  0.07055539164016725\n",
      "epoch :  890 train loss :  0.04308330883023315  val loss :  0.08257920042949378\n",
      "epoch :  891 train loss :  0.0429626001210893  val loss :  0.0708989088961971\n",
      "epoch :  892 train loss :  0.04368657753339613  val loss :  0.08105466287657458\n",
      "epoch :  893 train loss :  0.045097494653928344  val loss :  0.08994943770553483\n",
      "epoch :  894 train loss :  0.04319507782761963  val loss :  0.06857893026010582\n",
      "epoch :  895 train loss :  0.04618174938670316  val loss :  0.07318895437528776\n",
      "epoch :  896 train loss :  0.04461596235242814  val loss :  0.09318174577946602\n",
      "epoch :  897 train loss :  0.045503762779825645  val loss :  0.08444338946258603\n",
      "epoch :  898 train loss :  0.04239512163736121  val loss :  0.07190189696877208\n",
      "epoch :  899 train loss :  0.04458874659076411  val loss :  0.07001863616215535\n",
      "epoch :  900 train loss :  0.04113745483623437  val loss :  0.08257190386146897\n",
      "epoch :  901 train loss :  0.045384570449122585  val loss :  0.08770627535402424\n",
      "epoch :  902 train loss :  0.04255180687921521  val loss :  0.07278086649113344\n",
      "epoch :  903 train loss :  0.042993216390151  val loss :  0.07140289456525928\n",
      "epoch :  904 train loss :  0.04286975762684932  val loss :  0.07894924413274494\n",
      "epoch :  905 train loss :  0.042625786590326516  val loss :  0.07438419765079887\n",
      "epoch :  906 train loss :  0.04270975911960552  val loss :  0.07513537077523658\n",
      "epoch :  907 train loss :  0.044496424369396964  val loss :  0.08422181776772013\n",
      "epoch :  908 train loss :  0.044056086971653696  val loss :  0.07999444595540385\n",
      "epoch :  909 train loss :  0.0416842022744929  val loss :  0.06762273422833405\n",
      "epoch :  910 train loss :  0.042343471023736826  val loss :  0.07347604258854454\n",
      "epoch :  911 train loss :  0.043411896071586936  val loss :  0.08637760577165512\n",
      "epoch :  912 train loss :  0.043725587281335083  val loss :  0.07009783229043726\n",
      "epoch :  913 train loss :  0.04512506845304791  val loss :  0.06864057362020837\n",
      "epoch :  914 train loss :  0.04380130064698102  val loss :  0.08780868733137509\n",
      "epoch :  915 train loss :  0.0448174928339364  val loss :  0.07991141021899455\n",
      "epoch :  916 train loss :  0.041751109151393954  val loss :  0.06770850198532427\n",
      "epoch :  917 train loss :  0.045354762733840814  val loss :  0.07239737971610906\n",
      "epoch :  918 train loss :  0.04288767531748217  val loss :  0.08989610700526028\n",
      "epoch :  919 train loss :  0.04507126278824207  val loss :  0.08575425554317016\n",
      "epoch :  920 train loss :  0.041725755400994685  val loss :  0.07040029487487316\n",
      "epoch :  921 train loss :  0.045371163587685656  val loss :  0.06981286287421849\n",
      "epoch :  922 train loss :  0.04289726905173656  val loss :  0.07863167019694638\n",
      "epoch :  923 train loss :  0.044105844884252676  val loss :  0.09284086772693105\n",
      "epoch :  924 train loss :  0.045579624815761104  val loss :  0.08446775307142035\n",
      "epoch :  925 train loss :  0.04312016910517403  val loss :  0.07517432338975388\n",
      "epoch :  926 train loss :  0.04603370212759647  val loss :  0.06623538062025419\n",
      "epoch :  927 train loss :  0.045789962809981474  val loss :  0.07739015742946223\n",
      "epoch :  928 train loss :  0.04448730737478009  val loss :  0.0837346574788369\n",
      "epoch :  929 train loss :  0.04769995118154905  val loss :  0.09305802133426504\n",
      "epoch :  930 train loss :  0.04599417232133019  val loss :  0.0763325220781337\n",
      "epoch :  931 train loss :  0.04670741728931197  val loss :  0.06873257498062069\n",
      "epoch :  932 train loss :  0.04869611420868579  val loss :  0.07453090521953032\n",
      "epoch :  933 train loss :  0.04519199944681522  val loss :  0.07432524983607328\n",
      "epoch :  934 train loss :  0.04823361158722046  val loss :  0.08592011528158801\n",
      "epoch :  935 train loss :  0.04455377033750736  val loss :  0.08430441885311755\n",
      "epoch :  936 train loss :  0.04477129790795411  val loss :  0.07833450889338205\n",
      "epoch :  937 train loss :  0.04513357839113131  val loss :  0.07112729825807901\n",
      "epoch :  938 train loss :  0.04422672166055098  val loss :  0.07592609858217864\n",
      "epoch :  939 train loss :  0.04399665854676232  val loss :  0.08884137239770233\n",
      "epoch :  940 train loss :  0.0460854736766266  val loss :  0.08999256216518933\n",
      "epoch :  941 train loss :  0.043326403243535476  val loss :  0.07620123609304566\n",
      "epoch :  942 train loss :  0.0421700826821683  val loss :  0.06739203716711242\n",
      "epoch :  943 train loss :  0.04683527132954585  val loss :  0.06763152088992999\n",
      "epoch :  944 train loss :  0.045680008155747236  val loss :  0.07320748425479563\n",
      "epoch :  945 train loss :  0.04193140026084415  val loss :  0.07706805078573825\n",
      "epoch :  946 train loss :  0.04287846170911926  val loss :  0.07993758654586122\n",
      "epoch :  947 train loss :  0.04497760400683156  val loss :  0.08582815969717864\n",
      "epoch :  948 train loss :  0.04576533061515598  val loss :  0.08842460715931168\n",
      "epoch :  949 train loss :  0.04440608378792309  val loss :  0.07870490629413847\n",
      "epoch :  950 train loss :  0.04205460895184447  val loss :  0.0730761444979315\n",
      "epoch :  951 train loss :  0.04241641787373271  val loss :  0.07446426914262558\n",
      "epoch :  952 train loss :  0.04278223596864346  val loss :  0.06795200179498136\n",
      "epoch :  953 train loss :  0.04404498201780294  val loss :  0.07222378549009266\n",
      "epoch :  954 train loss :  0.04217114989007019  val loss :  0.08206638917390321\n",
      "epoch :  955 train loss :  0.042461708529379354  val loss :  0.07807662377063655\n",
      "epoch :  956 train loss :  0.04387405197774864  val loss :  0.08947230748991289\n",
      "epoch :  957 train loss :  0.04344272552087357  val loss :  0.07569061688033375\n",
      "epoch :  958 train loss :  0.04357640766075457  val loss :  0.07074969333174666\n",
      "epoch :  959 train loss :  0.043822253825074715  val loss :  0.0807474106388849\n",
      "epoch :  960 train loss :  0.043827850157053684  val loss :  0.06734626644347462\n",
      "epoch :  961 train loss :  0.04300669508532704  val loss :  0.08320548525368242\n",
      "epoch :  962 train loss :  0.04408810518838036  val loss :  0.08771828447940003\n",
      "epoch :  963 train loss :  0.04462315088518315  val loss :  0.0706350079537767\n",
      "epoch :  964 train loss :  0.04262759944141223  val loss :  0.08537710715220126\n",
      "epoch :  965 train loss :  0.04524268721917849  val loss :  0.0812963602178624\n",
      "epoch :  966 train loss :  0.04382739698816657  val loss :  0.06521337669997448\n",
      "epoch :  967 train loss :  0.045372082183810426  val loss :  0.07116665746492223\n",
      "epoch :  968 train loss :  0.04319148807157397  val loss :  0.08872655257602835\n",
      "epoch :  969 train loss :  0.04442898295234635  val loss :  0.07826415679099426\n",
      "epoch :  970 train loss :  0.045116540824711636  val loss :  0.0703020369464596\n",
      "epoch :  971 train loss :  0.0425666707192416  val loss :  0.07893124913063712\n",
      "epoch :  972 train loss :  0.04537437801585772  val loss :  0.07914591949469972\n",
      "epoch :  973 train loss :  0.04340506041744305  val loss :  0.0669346493370612\n",
      "epoch :  974 train loss :  0.0440490349678625  val loss :  0.07447618277833364\n",
      "epoch :  975 train loss :  0.04375860078445592  val loss :  0.09533203770648596\n",
      "epoch :  976 train loss :  0.04499280376662135  val loss :  0.07992518311235328\n",
      "epoch :  977 train loss :  0.04314674348302225  val loss :  0.0658567734357312\n",
      "epoch :  978 train loss :  0.04713241136706  val loss :  0.06727752558850121\n",
      "epoch :  979 train loss :  0.042987237874594034  val loss :  0.0821250869591702\n",
      "epoch :  980 train loss :  0.04401467710304323  val loss :  0.08525027103732469\n",
      "epoch :  981 train loss :  0.0418265783349881  val loss :  0.07931678977623204\n",
      "epoch :  982 train loss :  0.044346330053519206  val loss :  0.07554014518742892\n",
      "epoch :  983 train loss :  0.041848645979509304  val loss :  0.07416526423923533\n",
      "epoch :  984 train loss :  0.04328711085138521  val loss :  0.07425170513400163\n",
      "epoch :  985 train loss :  0.04505040647744821  val loss :  0.07083745298867972\n",
      "epoch :  986 train loss :  0.043100481121483895  val loss :  0.06821344529827802\n",
      "epoch :  987 train loss :  0.04325069076532781  val loss :  0.07168990117065507\n",
      "epoch :  988 train loss :  0.04142677887337994  val loss :  0.08253837417491877\n",
      "epoch :  989 train loss :  0.04243857348191488  val loss :  0.09096835395131067\n",
      "epoch :  990 train loss :  0.04394685445811736  val loss :  0.08720868303945362\n",
      "epoch :  991 train loss :  0.042347099827066144  val loss :  0.07758718677741673\n",
      "epoch :  992 train loss :  0.04348988193257941  val loss :  0.07420259588014791\n",
      "epoch :  993 train loss :  0.04129144327022642  val loss :  0.07880133086306015\n",
      "epoch :  994 train loss :  0.04267408062962337  val loss :  0.08100881213656544\n",
      "epoch :  995 train loss :  0.0420149355658686  val loss :  0.07579219647509444\n",
      "epoch :  996 train loss :  0.04146849532753073  val loss :  0.07069710078731348\n",
      "epoch :  997 train loss :  0.04341842490614085  val loss :  0.07167351515573221\n",
      "epoch :  998 train loss :  0.041963329265645034  val loss :  0.07756154687470344\n",
      "epoch :  999 train loss :  0.04340297500578521  val loss :  0.07734071021083522\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    random.shuffle(train_samples)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for i in range(0,len(train_samples),1024):\n",
    "        inp = []\n",
    "        out = []\n",
    "        for j in range(i,i+256):\n",
    "            if j < len(train_samples):\n",
    "                v1 = dataset[train_samples[j][0]][0,train_samples[j][1]:train_samples[j][2]]\n",
    "                v2 = dataset[train_samples[j][0]][1,train_samples[j][1]:train_samples[j][2]]\n",
    "                s = dataset[train_samples[j][0]][2,train_samples[j][1]:train_samples[j][2]]\n",
    "                \n",
    "                v1 = normalize(v1, -2**15, 2**15)\n",
    "                v2 = normalize(v2, -2**15, 2**15)\n",
    "                s = normalize(s, -2**15, 2**15)\n",
    "                \n",
    "                c1 = dataset[train_samples[j][0]][3,train_samples[j][1]:train_samples[j][2]]\n",
    "                c2 = dataset[train_samples[j][0]][4,train_samples[j][1]:train_samples[j][2]]\n",
    "                t = dataset[train_samples[j][0]][5,train_samples[j][1]:train_samples[j][2]]\n",
    "                \n",
    "                c1 = normalize(c1, -2**12, 2**12)\n",
    "                c2 = normalize(c2, -2**12, 2**12)\n",
    "                t = normalize(t, -2**10, 2**10)\n",
    "                \n",
    "                inp.append([v1,v2,s])\n",
    "                out.append([t])\n",
    "        inp = torch.from_numpy(np.asarray(inp)).cuda().float()\n",
    "        out = torch.from_numpy(np.asarray(out)).cuda().float()\n",
    "        \n",
    "        inp = inp.view(inp.size()[0], -1)\n",
    "        pred = net(inp)\n",
    "        \n",
    "        out = out.view(out.size()[0], -1)\n",
    "        \n",
    "        loss = criterion(pred, out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    train_loss = np.mean(train_losses)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    for i in range(0,len(val_samples),1024):\n",
    "        inp = []\n",
    "        out = []\n",
    "        for j in range(i,i+256):\n",
    "            if j < len(val_samples):\n",
    "                v1 = dataset[val_samples[j][0]][0,val_samples[j][1]:val_samples[j][2]]\n",
    "                v2 = dataset[val_samples[j][0]][1,val_samples[j][1]:val_samples[j][2]]\n",
    "                s = dataset[val_samples[j][0]][2,val_samples[j][1]:val_samples[j][2]]\n",
    "                \n",
    "                v1 = normalize(v1, -2**15, 2**15)\n",
    "                v2 = normalize(v2, -2**15, 2**15)\n",
    "                s = normalize(s, -2**15, 2**15)\n",
    "                \n",
    "                c1 = dataset[val_samples[j][0]][3,val_samples[j][1]+2]\n",
    "                c2 = dataset[val_samples[j][0]][4,val_samples[j][1]+2]\n",
    "                t = dataset[val_samples[j][0]][5,val_samples[j][1]+2]\n",
    "                \n",
    "                c1 = normalize(c1, -2**12, 2**12)\n",
    "                c2 = normalize(c2, -2**12, 2**12)\n",
    "                t = normalize(t, -2**10, 2**10)\n",
    "                \n",
    "                inp.append([v1,v2,s])\n",
    "                out.append([t])\n",
    "                \n",
    "        inp = torch.from_numpy(np.asarray(inp)).cuda().float()\n",
    "        out = torch.from_numpy(np.asarray(out)).cuda().float()\n",
    "        \n",
    "        inp = inp.view(inp.size()[0], -1)\n",
    "        pred = net(inp)\n",
    "        \n",
    "        out = out.view(out.size()[0], -1)\n",
    "        \n",
    "#         print (out.size(), pred.size())\n",
    "        loss = criterion(pred, out)\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "    val_loss = np.mean(val_losses)\n",
    "    \n",
    "    print ('epoch : ', epoch, 'train loss : ', train_loss, ' val loss : ', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_true = []\n",
    "c2_true = []\n",
    "t_true = []\n",
    "\n",
    "c1_pred = []\n",
    "c2_pred = []\n",
    "t_pred = []\n",
    "\n",
    "val_data = dataset['LM45_TorqueSteps.mat']\n",
    "for i in range(0,val_data.shape[1],1):\n",
    "    if i + 5< val_data.shape[1]:\n",
    "        v1 = val_data[0,i:i+5]\n",
    "        v2 = val_data[1,i:i+5]\n",
    "        s = val_data[2,i:i+5]\n",
    "\n",
    "        v1 = normalize(v1, -6146, 6753)\n",
    "        v2 = normalize(v2, -15860, 16335)\n",
    "        s = normalize(s, -13149, 13867)\n",
    "\n",
    "        c1 = val_data[3,i+2]\n",
    "        c2 = val_data[4,i+2]\n",
    "        t = val_data[5,i+2]\n",
    "\n",
    "        c1 = normalize(c1, -971, 2159)\n",
    "        c2 = normalize(c2, -2900, 2988)\n",
    "        t = normalize(t, -6442, 6456)\n",
    "        \n",
    "        c1_true.append(c1)\n",
    "        c2_true.append(c2)\n",
    "        t_true.append(t)\n",
    "        \n",
    "        inp = [v1,v2,s]\n",
    "        \n",
    "        inp = torch.from_numpy(np.asarray(inp)).cuda().float().view(1,-1)\n",
    "        \n",
    "#         print (inp.size())\n",
    "        pred = net(inp).data.cpu().numpy()\n",
    "        \n",
    "#         c1_pred.append(pred[0,0])\n",
    "#         c2_pred.append(pred[1,0])\n",
    "        t_pred.append(pred[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f25ff625080>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_true)\n",
    "plt.plot(t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LM10.mat', 'LM30.mat', 'LM60_TroqueSteps.mat', 'LM50.mat', 'NoLM.mat', 'NoLM_SpeedVariations.mat', 'LM68.mat', 'NoLM_SpeedVariations2.mat', 'LM20_TorqueSteps.mat', 'LM45_TorqueSteps.mat'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
