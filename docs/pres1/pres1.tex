\documentclass[handout]{beamer}
%\documentclass [serif,mathserif,professionalfont]{beamer}
%\usepackage {pxfonts}
%\usepackage {eulervm}
%\usepackage{mathpazo}
%\logo{\includegraphics[height=1.2cm]{fsulogo.png}}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subfig,fixltx2e,url}
\usepackage[latin1]{inputenc}
\usepackage{pgfplots}
\usepackage{hyperref}
\usepackage{booktabs}

%\usetheme{Warsaw}
\usecolortheme{lily}
\setbeamercovered{transparent}
%\useoutertheme[subsection=false]{smoothbars}

\setbeamertemplate{sidebar right}{}
\setbeamertemplate{footline}{%
\hfill\usebeamertemplate***{navigation symbols}
\hspace{1cm}\insertframenumber{}/\inserttotalframenumber}

\title[ \insertdate]{Multi-Input Multi-Output Electric Motor Signal Prediction using Neural Networks}

\author{Sagar Verma}
\institute[Centralesup\'elec and Schneider Electric] % (optional, but mostly needed)
{
  Centre de Vision Num\'erique,\\
  Centralesup\'elec, Gif-sur-Yvette}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{November, 2018}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Table Of Contents}
\begin{enumerate}
\item Problem Statement
\item Background
\item Dataset and Experiments
\item Results and Conclusions
\end{enumerate}
\end{frame}

\section{Problem Statement}

\begin{frame}{Problem Statement}
\begin{center}
\begin{enumerate}
  \item Input: multiple signals
  \item Output: multiple signals.
  \item Continuous signals: regression problem.
\end{enumerate}
\vspace{.5cm}
\includegraphics[width=1\linewidth]{images/teaser}
\end{center}
\end{frame}


\begin{frame}{Problem Statement}{Challenges}
  \begin{enumerate}
    \item How to handle continuous signals?
    \item How to handle long sequences? Electric motors generally operate for long hours.
    \item Which prediction model to use?
    \item How to handle multiple outputs?
  \end{enumerate}
\end{frame}

\section{Background}
\begin{frame}{Background}
  \begin{enumerate}
    \item Perceptrons
    \item Feed-forward Networks
    \item Activation Functions
    \item Backpropagation
    \item Loss Functions
    \item Sequential Networks
    % \item Generative Networks
  \end{enumerate}
\end{frame}

\begin{frame}{Background}{Perceptrons}
\begin{enumerate}
  \item Basic block of neural networks \\

  \item Input: binary variables, $x_i$ \\

  \item Output: Binary decision, $y$ \\

  \item $w_i$ is a weight, the output is calculated using \\

  \begin{eqnarray}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      0 & \mbox{if } \sum_i w_i x_i \leq \mbox{ threshold} \\
      1 & \mbox{if } \sum_i w_i x_i > \mbox{ threshold}
      \end{array} .
\end{eqnarray}
\end{enumerate}
    \begin{center}
      \includegraphics[width=0.8\linewidth, height=4cm]{images/perceptron}
    \end{center}
\end{frame}

\begin{frame}{Background}{Feed-forword Networks}
\begin{enumerate}
  \item AKA Artificial Neural Networks (ANNs) \\

  \item Multiple stacked perceptrons \\

  \item \#connections increases with \#perceptrons \\

  \item Learning weights is difficult
    \begin{enumerate}
      \item Binary output does not tell us which weights are important and which are not \\
      \item A small change in input or weight could flip the output from 0 to 1 or vice versa \\
    \end{enumerate}
\end{enumerate}
    \begin{center}
      \includegraphics[width=0.65\linewidth, height=3.5cm]{images/anns}
    \end{center}
\end{frame}

\begin{frame}{Background}{Activation Functions}
  \begin{enumerate}
    \item To learn weights of an ANN we need continuous output. \\

    \item Function that can give small changes in output when small changes in weights are made. \\

    \item Should give a smooth output. \\

    \item $f$ should be non-linear, $y = f(w^Tx+b)$ \\
  \end{enumerate}
  \begin{center}
    \includegraphics[width=0.65\linewidth, height=3.5cm]{images/anns_change}
  \end{center}
\end{frame}

\begin{frame}{Background}{Activation Functions: Examples}


\begin{enumerate}

  \item Sigmoid: $ f(x) = \sigma(x) = \frac{1}{1+\exp(-\sum_i w_i x_i)}$ \\

  \item Tanh: $ f(x) = tanh(x) = \frac{\exp(\sum_i w_i x_i) - \exp(-\sum_i w_i x_i)}{\exp(\sum_i w_i x_i) + \exp(-\sum_i w_i x_i)}$ \\

  \item Rectified Linear Unit (ReLU): $f(x) = \left\{\begin{array}{ll}
      0 & \mbox{if } \sum_i w_i x_i < \mbox{ 0} \\
      x & \mbox{if } \sum_i w_i x_i \geq \mbox{ 0}
      \end{array}$ \\
\end{enumerate}

\vspace{0.5cm}

\begin{center}
      \resizebox{2cm}{2cm}{
        \begin{tikzpicture}
          \begin{axis}%
          [
              grid=major,
              xmin=-6,
              xmax=6,
              axis x line=bottom,
              ytick={0,.5,1},
              ymax=1,
              axis y line=middle,
          ]
              \addplot%
              [
                  blue,%
                  mark=none,
                  samples=100,
                  domain=-6:6,
              ]
              (x,{1/(1+exp(-x))});
          \end{axis}
        \end{tikzpicture}}
      \resizebox{2cm}{2cm}{
        \begin{tikzpicture}
          \begin{axis}%
          [
              grid=major,
              xmin=-6,
              xmax=6,
              axis x line=bottom,
              ytick={-.5,0,.5,1},
              ymax=1,
              axis y line=middle,
          ]
              \addplot%
              [
                  blue,%
                  mark=none,
                  samples=100,
                  domain=-6:6,
              ]
              (x,{(exp(x)-exp(-x))/(exp(x)+exp(-x))});
          \end{axis}
        \end{tikzpicture}}
      \resizebox{2cm}{2cm}{
        \begin{tikzpicture}
          \begin{axis}%
          [
              grid=major,
              xmin=-6,
              xmax=6,
              axis x line=bottom,
              ytick={-5,1,2,3,4,5},
              ymax=5,
              axis y line=middle,
          ]
              \addplot%
              [
                  blue,%
                  mark=none,
                  samples=100,
                  domain=-6:6,
              ]
              (x, ifthenelse(x<=0,0,x));
          \end{axis}
        \end{tikzpicture}}

        \small{Sigmoid, Tanh, and ReLU}
\end{center}
\end{frame}


\begin{frame}{Background}{Loss Functions}
\begin{enumerate}
  \item Tells us how good or bad network is. \\

  \item \textbf{L1 Loss} $\mathcal{L}(y_{true},y_{pred}) = |y_{true}-y_{pred}|$ \\

  \item \textbf{MSE Loss} $\mathcal{L}(y_{true},y_{pred}) = (y_{true}-y_{pred})^2$ \\

  \item \textbf{Cross Entropy Loss} (classification) $\mathcal{L}(y_{true},y_{pred}) = -\sum_{c=1}^{C}y^c_{true}log(y^c_{pred})$ \\
\end{enumerate}
\end{frame}

\begin{frame}{Background}{}
    \center
    \Large{\color{blue}We have a network (ANN + activation function). \\
    We can judge it (loss function). \\
    \textit{How do we learn weights?}}
\end{frame}

\begin{frame}{Background}{Learning Weights}
  \begin{enumerate}
    \item Small loss means good weights.
    \item Learn weights using \textit{Optimizers}.
  \end{enumerate}
\end{frame}

% \begin{frame}{Backpropagation}
%   \begin{enumerate}
%     \item Forward propogate input to get output from a network.
%     \item Calculate loss.
%     \item Backpropagate the loss(error) to all the weights.
%     \item Each weight's output and input activation are mutliplied to find the gradient of the weight.
%     \item A ratio of the weight's gradient is subtracted from the weight.
%   \end{enumerate}
% \end{frame}

\begin{frame}{Background}{Optimizers}
  \begin{enumerate}
    \item Can not process large dataset at once.
    \item Process data in small parts (mini-batches).
    \item Optimizers for learning weights using mini-batches.
    \item Stochastic Gradient Descent is the most used optimizer.
  \end{enumerate}
\end{frame}

\begin{frame}{{Background}}
  \center
  \Large{\color{blue}We can now train an ANN. \\
  Can we process continous signals?}
\end{frame}

\begin{frame}{Background}{Sequential Networks}
  \begin{enumerate}
    \item ANNs can be used by splitting sequences.
    \item Sequence networks for temporal learning.
    \item Recurrent neural networks (RNNs) and Long-Short Term Memorry (LSTM).
  \end{enumerate}
  \begin{center}
    \begin{figure}
    \includegraphics[width=0.6\linewidth, height=3.5cm]{images/RNN}
    \caption{RNN unrolled in time.}
    \end{figure}
  \end{center}
\end{frame}

\begin{frame}{Background}{Sequential Networks: Recurrent Neural Networks}
  \begin{enumerate}
    \item Perform same task for every element of a sequence.
    \item Output depends on previous elements.
    \item RNNs can be seen as a neural network having "memory".
    \begin{equation}
        h_t = tanh(Wx_t+Uh_{t-1}),
    \end{equation}
    where $W$ and $U$ are weights, $h$ is the hidden vector and $x_t$ is the input at time $t$.
  \end{enumerate}

\end{frame}

\begin{frame}{Background}{Sequential Networks: Long-Short Term Memory}
  \begin{enumerate}
    \item RNNs have vanishing and exploding gradients problem.
    \item LSTM resolves above problems.
    \item Computes when to forget and when to remember.
    % \begin{center}
    %   \begin{figure}
    %   \includegraphics[width=0.6\linewidth, height=4cm]{images/lstm}
    %   \caption{LSTM Cell.}
    %   \end{figure}
    % \end{center}
  \end{enumerate}
\end{frame}

% \begin{frame}{Generative Networks}
%   \begin{enumerate}
%
%   \end{enumerate}
% \end{frame}

\begin{frame}{Dataset and Experiments}
  \center\Large{\color{blue}Neural networks for motor control}
\end{frame}

\section{Dataset}
\begin{frame}{Dataset and Experiments}{Dataset}
  \begin{enumerate}
    \item \textit{CNC Mill Tool Wear} dataset from \href{https://www.kaggle.com/shasun/tool-wear-detection-in-cnc-mill}{\color{blue}\textit{kaggle}}.
    \item Provides real world motor speed.
    \item 18 experiments under different conditions.
    \item Mean experiment time is 136.6389 seconds.
    \item 14 experiments are used for training and 4 for testing.
  \end{enumerate}
\end{frame}

\section{Experiments and Results}
\begin{frame}{Dataset and Experiments}{Experimental Setup}
  \begin{enumerate}
    \item Simulink model generates voltages, currents and torque.
    \item PyTorch for network implementation.
    \item Dataset scaled between $(-1,1)$.
    \item Simulink gives data at 20KHz, downsample to 100Hz.
    \item Window size, $w$: 30, 60, and 100 at stride $s$: 10.
    \item Mean Square Error (MSE) to evaluate.
  \end{enumerate}
\end{frame}

\begin{frame}{Dataset and Experiments}{ANN for signal prediction}
  \begin{enumerate}
    \item Three outputs, three networks.
    \item Input: $w\times3$ 1-D vector, Output: $w$
    \item Activation, $f$: tanh
  \end{enumerate}
  \begin{center}
    \begin{figure}
    \includegraphics[scale=0.35]{images/motor_ann}
    \end{figure}
  \end{center}
\end{frame}

\begin{frame}{Dataset and Experiments}{LSTM for signal prediction}
  \begin{enumerate}
    \item Three outputs, three networks.
    \item Input: $w\times3$ 2-D vector, Output: $w$
    \item Activation, $f$: tanh
  \end{enumerate}
  \begin{center}
    \begin{figure}
    \includegraphics[scale=0.35]{images/motor_lstm}
    \end{figure}
  \end{center}
\end{frame}

\begin{frame}{Dataset and Experiments}{Multi-Output LSTM model}
\begin{enumerate}
  \item Input: $w\times3$ 2-D vector, Output: $w\times3$
  \item Activation, $f$: tanh
\end{enumerate}
\begin{center}
  \begin{figure}
  \includegraphics[scale=0.35]{images/motor_comb_lstm}
  \end{figure}
\end{center}
\end{frame}

\begin{frame}{Results and Conclusions}{Best Model}
\begin{table}[]
  \begin{tabular}{l c c c c}
  \toprule[0.2mm]
   \textbf{Model} & \textbf{w} & \textbf{Current1} & \textbf{Current2} & \textbf{Torque}\\
   \midrule
   & 100 & 0.072 & 0.197 & 0.672 \\
   \textbf{ANN} & 60 & 0.043 & 0.105 & 0.564 \\
   & 30 & \textbf{0.031} & \textbf{0.091} & \textbf{0.056} \\
   \midrule
   & 100 & 0.146 & 0.182 & 0.723 \\
   \textbf{LSTM} & 60 & 0.136 & 0.107 & 0.688 \\
   & 30 & 0.045 & 0.105 & 0.072 \\
   \midrule
   \textbf{MO-LSTM} & 60 & 0.139 & 0.112 & 0.691 \\
   & 30 & 0.051 & 0.109 & 0.081 \\
   \bottomrule
  \end{tabular}
  \caption{MSE of different models with different sequence lengths.}
  \end{table}
\end{frame}
% \begin{frame}{RBM for signal prediction}
%
% \end{frame}

\begin{frame}{Results and Conclusions}{Model Convergence}
\begin{center}
  \begin{figure}
  \includegraphics[scale=0.25]{images/ann30_curr1}
  \includegraphics[scale=0.25]{images/ann30_curr2}
  \includegraphics[scale=0.25]{images/ann30_torque}
  \end{figure}
\end{center}
\end{frame}

\begin{frame}{Results and Conclusions}{Example Outputs}
\begin{center}
  \begin{figure}
  \includegraphics[scale=0.2]{images/curr1_exp13_epoch190_true}
  \includegraphics[scale=0.2]{images/curr2_exp13_epoch190_true}
  \includegraphics[scale=0.2]{images/torque_exp13_epoch190_true}\\
  \includegraphics[scale=0.2]{images/curr1_exp13_epoch190_pred}
  \includegraphics[scale=0.2]{images/curr2_exp13_epoch190_pred}
  \includegraphics[scale=0.2]{images/torque_exp13_epoch190_pred}
  \center{\caption{Top row: ground truth, bottom row: predicted signal, \\ left to right: current1, current2, and torque}}
  \end{figure}
\end{center}
\end{frame}

\section{Conclusions and Future Work}
\begin{frame}{Results and Conclusions}{Some Answers}
\begin{enumerate}
  \item How to handle continous signals? \textbf{Use tanh}
  \item How to handle long sequences? \textbf{Find optimal subsequence}
  \item Which prediction model to use? \textbf{ANN}
  \item How to handle multiple outputs? \textbf{Independent Models}
\end{enumerate}
\end{frame}

\begin{frame}
\center
\color{blue}
\huge{Thank you!}\\
\huge{Questions?}\\
\end{frame}

\end{document}
